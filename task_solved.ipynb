{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Text classification with a 2-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 1. Task description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Text classification with neural networks\n",
    "In the following sections, you will implement a simple neural network for text classification and train it on the [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/). \n",
    "The goal of text classification is, given an input text, to assign to it one of $K$ mutually exclusive labels. The most basic neural network consists of several feedforward fully-connected layers. Since text is a sequence of tokens (or characters), the most commonly used neural networks in NLP are [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network), which learn well from sequential data. However, regular feedforward neural networks are still widely used as baselines or parts of more complex models. In this assignment, you will implememnt a simple feed forward neural network and train it with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) on a subset of the [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 20 Newsgroups dataset\n",
    "This dataset is a popular benchmark for classification models. It consists of $20000$ newsgroups documents, where each document has an associated label, selected out of $K=20$ classes (different newsgroups). Each class has roughly the same number of documents, making this dataset balanced. For for faster computation, here we will only use a small subset of classes, which consists of the following newsgroups: `comp.os.ms-windows.misc`, `rec.motorcycles`, `sci.space`, and `talk.politics.misc`. Each group contains roughly training $500$ samples. Thus, in total we will have a dataset with $4$ classes and slightly over $2000$ samples.\n",
    "\n",
    "[Scikit-learn library](https://scikit-learn.org) has a convenient way of loading the 20 Newsgroups dataset, so we will use it to fetch the dataset and look at the document representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to make sure your environment is fully set up. You are free to import any additional [standard Python libraries](https://docs.python.org/3/library/), however, the notebook can be completed without doing so. External libraries other than the ones listed below are not permitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import papermill as pm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['comp.os.ms-windows.misc', 'rec.motorcycles', 'sci.space', 'talk.politics.misc', ]\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories, data_home='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 2247\n",
      "Categories: 4\n"
     ]
    }
   ],
   "source": [
    "print('Samples:', len(dataset.data))\n",
    "print('Categories:', len(dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Let's print a random example to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: rec.motorcycles \n",
      "\n",
      "From: rbemben@timewarp.prime.com (Rich Bemben)\n",
      "Subject: Re: April 1( was Re: FAQ - What is the DoD?)\n",
      "Expires: 30 Apr 93 05:00:00 GMT\n",
      "Organization: Computervision Corp., Bedford, Ma.\n",
      "Lines: 31\n",
      "\n",
      "In article <9901221@hpfcso.FC.HP.COM> jld@hpfcso.FC.HP.COM (Jeff Deeney) writes:\n",
      ">In rec.motorcycles, viking@iastate.edu (Dan Sorenson) writes:\n",
      ">\n",
      ">> Last year, I believe it was, Jeff Deeney posted what I've since come to\n",
      ">> recognize as the ultimate April Fools posting ever.  It wasn't particularly\n",
      ">> nice of him, as several people were quite fooled and very worried about\n",
      ">> him, but I can't fault the effectiveness.\n",
      ">\n",
      ">Based on numerous inputs (most of them unprintable), I deemed it time for a\n",
      ">kinder, gentler, April 1.  Not that I didn't have something really sick and\n",
      ">twisted ready to post :-)  Perhaps next year. \n",
      "\n",
      "Personally, I think Jeffy-Poo was still smarting more from the third degree\n",
      "burns he suffered after April 1st last year rather than the supposed burns\n",
      "that he suffered in \"the joke\".  Granted I was one of those people that were\n",
      "taken in by it and I was certainly concerned...and then pissed at him for \n",
      "pulling such a thing (which I made known to him).\n",
      "\n",
      "But then again, for an April Fool \"joke\" I would also go on record as saying\n",
      "that it was the best orchestrated one I've ever seen and it certainly sucked\n",
      "a LOT of people into believing it 8-( 8-| 8-\\ 8-)...\n",
      "\n",
      "\"sick\" - \"twisted\"??? Who in this group could ever be accused of such a thing?\n",
      "\n",
      "I tip my twisted lid to thee Jeffy 8-).\n",
      "\n",
      "\n",
      "Rich Bemben - DoD #0044                     rbemben@timewarp.prime.com\n",
      "1977 750 Triumph Bonneville                 (617) 275-1800 x 4173\n",
      "\"Fear not the evil men do in the name of evil, but heaven protect\n",
      " us from the evil men do in the name of good\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sample_idx = random.randrange(len(dataset.data))\n",
    "print('Label:', dataset.target_names[dataset.target[random_sample_idx]], '\\n')\n",
    "print(dataset.data[random_sample_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Text representation for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "As you can see, the documents are quite long and diverse. However, documents within each class contain some specific words that are less likely to be used in the documents from other classes. Our model will leverage this fact to perform text classification. In order to do that, we need to convert each document into numeric form. One of the simplest and most popular representations in NLP is the *multi-hot representation*. In this representation, each document $d$ is represented as a vector $\\mathbf{x}$ of length $|V|$, where $V$ is the vocabulary - that is, a set of all possible words $v$, used in the whole dataset. The vector $\\mathbf{x}$ contains $1$ on the $i$th position if the corresponding word $v_i$ was used in the document $d$, and $0$ otherwise.\n",
    "\n",
    "Finally, we would need to convert the labels (`dataset.target`) to a matrix of one-hot vectors to use it to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The cell below will be used for unit tests. Please do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "docs_toy = [\n",
    "\"\"\"\n",
    "Hi!\n",
    "\n",
    "How are you?\n",
    "\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Do you have a dog?\n",
    "\"\"\"\n",
    "]\n",
    "docs_toy_labels = np.array([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document: \n",
      "Hi!\n",
      "\n",
      "How are you?\n",
      "\n",
      "\n",
      "Second document: \n",
      "Do you have a dog?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('First document:', docs_toy[0])\n",
    "print('Second document:', docs_toy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      " [0 1]\n"
     ]
    }
   ],
   "source": [
    "print('Labels:\\n', docs_toy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "### 1.1 \n",
    "#### (2 points)\n",
    "\n",
    "Complete the code in the `tokenize_doc` function that returns a list of tokens for a given document.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- You can use the `string.punctuation` to get the punctuation characters\n",
    "- You can use the `string.whitespace`  to get all whitespace characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_doc(doc):\n",
    "    \"\"\"\n",
    "        Convert the input document into a list of tokens, discarding all punctuation and lowercasing the tokens\n",
    "        doc: string\n",
    "\n",
    "        return list of strings\n",
    "    \"\"\"\n",
    "    # discard all punctuation\n",
    "\n",
    "    # replace all whitespace characters with just space\n",
    "\n",
    "    # split doc into tokens by space\n",
    "\n",
    "    # discard empty tokens and lowercase\n",
    "\n",
    "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in doc.split():\n",
    "        if token in string.whitespace or token == '':\n",
    "            continue\n",
    "        tokens.append(token.lower())\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_toy_tokenized = [tokenize_doc(d) for d in docs_toy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "If you implememnted this function correcrtly, the documents would look like this:\n",
    "```\n",
    "['hi', 'how', 'are', 'you']\n",
    "```\n",
    "```\n",
    "['do', 'you', 'have', 'a', 'dog']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document tokenized:\n",
      " ['hi', 'how', 'are', 'you']\n",
      "Second document tokenized:\n",
      " ['do', 'you', 'have', 'a', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print('First document tokenized:\\n', docs_toy_tokenized[0])\n",
    "print('Second document tokenized:\\n', docs_toy_tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "tokenization.test_docs.0": [
        "hi",
        "how",
        "are",
        "you"
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "tokenization.test_docs.1": [
        "do",
        "you",
        "have",
        "a",
        "dog"
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('tokenization.test_docs.0', docs_toy_tokenized[0])\n",
    "pm.record('tokenization.test_docs.1', docs_toy_tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1.2\n",
    "#### (1 point)\n",
    "\n",
    "Complete the code in `build_vocab`. If the `min_count` argument is `None`, you should not discard any tokens.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- Hint: you might find the `Counter` class from the `collections` library and `from_iterable` method from `itertools.chain` useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(docs, min_count=None):\n",
    "    \"\"\"\n",
    "        Build the vocaublary mapping (that is, the correspondance between the token and its numeric id)\n",
    "        docs: a list of tokenized documents\n",
    "        min_count (optional): int, discard tokens that appeared less than min_count times\n",
    "\n",
    "        return dictionary str -> int\n",
    "    \"\"\"\n",
    "\n",
    "    # count all tokens in all documents and filter those that appear less than min_count\n",
    "    docs = list(itertools.chain.from_iterable(docs))\n",
    "    docs.sort()\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "    \n",
    "    if min_count is not None:\n",
    "        counter = Counter()\n",
    "        for token in docs: counter[token] += 1\n",
    "        for token in counter.keys():\n",
    "            if counter[token] < min_count: continue\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    else:\n",
    "        for token in docs:\n",
    "            if token in vocab.keys(): continue\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    \n",
    "    # create the vocabulary mapping\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(docs_toy_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "If you implememnted this function correcrtly, the vocabulary would contain 8 tokens and similar to the following:\n",
    "```\n",
    "{'a': 0, 'are': 1, 'do': 2, 'dog': 3, 'have': 4, 'hi': 5, 'how': 6, 'you': 7}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'a': 0, 'are': 1, 'do': 2, 'dog': 3, 'have': 4, 'hi': 5, 'how': 6, 'you': 7}\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary:\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "vocab.test_docs": {
        "a": 0,
        "are": 1,
        "do": 2,
        "dog": 3,
        "have": 4,
        "hi": 5,
        "how": 6,
        "you": 7
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('vocab.test_docs', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1.3 \n",
    "#### (1 point)\n",
    "\n",
    "Complete the code in the `doc_to_multihot` function that tranforms the input document to its' multi-hot representation. \n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- Recall that multi-hot representation of a given document should return a vector containing ones at the positions of the words present in the document and zeros at all the other positions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def doc_to_multihot(doc, vocab):\n",
    "    \"\"\"\n",
    "        Convert a document to a multihot representation\n",
    "        doc: str, a tokenized document\n",
    "        vocab: dict, vocabulary mapping\n",
    "\n",
    "        return np.array, shape=(|V|,)\n",
    "    \"\"\"\n",
    "\n",
    "    # create a vector of zeros of the shape (|V|, )\n",
    "    x = np.zeros(len(vocab))\n",
    "\n",
    "    # set the corresponding dimensions to 1\n",
    "    for token in doc: \n",
    "        if token in vocab.keys(): x[vocab[token]] = 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_toy_multi_hot = [doc_to_multihot(doc, vocab) for doc in docs_toy_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "If you implemented this function correcrtly, the documents in multi-hot representation would look similar to the following\n",
    "```\n",
    "[0. 1. 0. 0. 0. 1. 1. 1.]\n",
    "```\n",
    "```\n",
    "[1. 0. 1. 1. 1. 0. 0. 1.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Notice how these documents have $1$ in the dimensions matching to the tokens indices from the vocabulary. In particular, since both documents have the token `you`, they both have $1$ in the corresponding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document multi-hot:\n",
      " [0. 1. 0. 0. 0. 1. 1. 1.]\n",
      "Second document multi-hot:\n",
      " [1. 0. 1. 1. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('First document multi-hot:\\n', docs_toy_multi_hot[0])\n",
    "print('Second document multi-hot:\\n', docs_toy_multi_hot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "multihot.test_docs.0": [
        0,
        1,
        0,
        0,
        0,
        1,
        1,
        1
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "multihot.test_docs.1": [
        1,
        0,
        1,
        1,
        1,
        0,
        0,
        1
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('multihot.test_docs.0', docs_toy_multi_hot[0].tolist())\n",
    "pm.record('multihot.test_docs.1', docs_toy_multi_hot[1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 1.4 \n",
    "#### (2 points)\n",
    "\n",
    "Because later on it will be easier to work with true labels as with one-hot encoded vectors rather than with single integers, you will now need to complete the code in `labels_to_onehot` that converts a vector of class labels to a one-hot representation.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- For example, if there were $k=3$ classes in your dataset and the true labels vector looked like $\\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$ (assuming there were $N=4$ examples), then the ultimate one-hot representation of the labels would be the following: $\\mathbf{y} = \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0\\end{bmatrix}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def labels_to_onehot(labels):\n",
    "    \"\"\"\n",
    "        Convert the indices to one-hot representation\n",
    "        labels: np.array of labels, shape=(N,)\n",
    "\n",
    "        return np.array, shape=(N, k)\n",
    "       \"\"\"\n",
    "\n",
    "    n_classes = len(set(labels))\n",
    "    n_samples = len(labels)\n",
    "\n",
    "    # create a matrix of zeros of shape (n_samples, n_classes)\n",
    "    one_hot = np.zeros((n_samples, n_classes))\n",
    "\n",
    "    # fill one-hot values\n",
    "    label_mapping = {}\n",
    "    for index, label in enumerate(set(labels)): label_mapping[label] = index \n",
    "    for index, label in enumerate(labels): one_hot[index, label_mapping[label]] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_toy_labels_onehot = labels_to_onehot(docs_toy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "If you implemented this function correcrtly, the one-hot labels would look like this:\n",
    "```\n",
    "[[1. 0.]\n",
    " [0. 1.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels one-hot:\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print('Test labels one-hot:\\n', docs_toy_labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "onehot.test_labels": [
        [
         1,
         0
        ],
        [
         0,
         1
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('onehot.test_labels', docs_toy_labels_onehot.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 2. Neural networks overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Introduction\n",
    "Neural networks (NNs) are machine learning alrogithms that are designed to capture patterns in a way similar to what human brain does. NNs are very efficient at discovering latent structures in unlabeled data and automatically extracting (or learning) relevant features.\n",
    "\n",
    "In principle, there exist various NN architectures that are commonly applied to different problems. Essentially, every neural network consists of a set of computational units called neurons. Every neuron processes the input signal, computes the output, and passes it further to the next neuron. Altogether neurons can recognize complex patterns. Our current task will be focused on the simplest type of a neural network - the so-called fully-connected neural network.\n",
    "\n",
    "### Fully-connected neural network\n",
    "Fully-connected neural networks can be viewed as a stack of layers, where every layer consists of an arbitrary number of neurons. Conventionally, the first layer is called an input layer, all the middle layers are referred to as hidden layers, and the last layer is called the output layer. The output layer is responsible for making ultimate high-level decisions and generating the predictions. The diagram of the feed-forward neural network with one layer (it is common to only count the hidden layers):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "<center><img src=\"data:image/png;base64,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\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "The depth of a neural network is one of its key features. Each node of a single layer combines the outputs of the nodes in the previous layer and adds them up in different proportions, i.e. weights, and applies a non-linear function to the obtained value. Non-linearity is important, since a stack of linear layers without non-linear activations would be equivalent to just one single linear layer, which reduces the capacity of a neural network to recover the elaborate structures in the input data. Formally, if we denote the weights and the biases of the $i$-th layer as matrices $W^i$ and $\\mathbf{b}$ respectively, and the input example feature vector as $\\mathbf{x}$, then the $i$-th linear layer's transformation is as follows:\n",
    "\n",
    "\n",
    "$$ \\mathbf{z}^{(i)} = W^{(i)}\\mathbf{x} + \\mathbf{b} $$\n",
    "$$ \\mathbf{a}^{(i)} = f\\left(\\mathbf{z}^{(i)}\\right)$$\n",
    "\n",
    "where $f$ is the mentioned non-linear function called the activation function. There is a number of activation functions that are widely used in Deep Learning. In classification tasks, the dimension of the last layer equals the number of classes $k$ and the activation function of the last layer is, typically, either the sigmoid function (for binary classification problmes) or the softmax function (which is a generalization of the sigmoid function over multiclass classification problems).\n",
    "\n",
    "### Softmax function and predictions\n",
    "Given a vector $\\mathbf{z}$ of the dimension $k$, the softmax function computes the vector with the following components:\n",
    "\n",
    "$$ \\text{softmax}\\left(\\mathbf{z}\\right)_i = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}} $$\n",
    "\n",
    "In other words, the softmax function first exponentiates the vector (elementwise) and then normalizes it such that all the components would add up to 1. In the output layer the resulting vector can be interpreted as a probability distribution over the number of classes. The network then makes its prediction through selecting a class with the maximum associated probability:\n",
    "\n",
    "$$ \\hat{y}_{pred} = \\underset{j \\in 1..k}{\\text{argmax}}\\left[\\text{softmax}\\left(\\mathbf{z}\\right)\\right] $$\n",
    "\n",
    "One interesting property of the softmax function is that it is invariant to constant offsets, i.e. $\\text{softmax}\\left(\\mathbf{z}\\right) = \\text{softmax}\\left(\\mathbf{z} + \\mathbf{c}\\right)$, where $\\mathbf{c}$ is a broadcasted vector of equal constant values. \n",
    "\n",
    "To sum up, for a single-layer neural network the predicted probability distributions are computed as:\n",
    "\n",
    "$$ \\mathbf{\\hat{y}} = \\text{softmax}\\left(W\\mathbf{x} + \\mathbf{b}\\right) $$\n",
    "\n",
    "### Training\n",
    "Simply put, training of a neural network reduces to learning its preliminarily initialized weights such that the learned weights would optimize a given objective. The most popular optimization algorithm used in Deep Learning is gradient descent. The training objective is usually formulated as maximization of the likelihood of the training data, which for classification tasks is equivalent to minimizing the cross-entropy function:\n",
    "\n",
    "$$ \\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N y_n\\log \\hat{y}_n \\rightarrow \\min_{W, b}$$\n",
    "\n",
    "where $\\hat{y}_n$ is the predicted probability of the correct class $y_n$ for the $n$-th training example and $N$ is the total number of training examples. In order to reduce overfitting, an additional term penalizing large weights is added to the loss function as:\n",
    "\n",
    "$$ \\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N y_n\\log \\hat{y}_n + \\lambda \\Vert W\\Vert ^2 \\rightarrow \\min_{W, b}$$\n",
    "\n",
    "\n",
    "The actual training is done in iterations such that after every iteration the network produces the predictions, computes the value of the loss function based on the predictions and the ground-truth labels, calculates the gradients through a technique called backpropagation, and then uses the derived gradients to update the weights (a step of the gradient descent algorithm). Backpropagation is the heart of neural networks algorithm; it calculates the gradients of the loss function with respect to every trained weight through application of a series of chain rules. According to the gradient descent algorithm, the weights of the network are updated as:\n",
    "\n",
    "$$ W_{ij} \\leftarrow W_{ij} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}} $$\n",
    "\n",
    "$$ b_{i} \\leftarrow b_{i} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial b_{i}} $$\n",
    "\n",
    "where $\\alpha$ is a hyperparameter called learning rate that adjusts the magnitude of the weight updates.\n",
    "\n",
    "In practice, training is done in batches: instead of processing one training instance at a time, a network receives multiple examples which are processed in parallel. Besides the computation efficiency, this approach also produces a better estimate of the gradients.\n",
    "\n",
    "---\n",
    "## Now let's code it up!\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 3. Step-by-step single-layer network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The cell below will be used for unit tests. Please do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# 5 features x 3 classes\n",
    "W_toy = np.array([[0.2, -0.3, 0.5],\n",
    "                 [-0.9, 0.6, 0.4],\n",
    "                 [0.1, 0.7, -0.2],\n",
    "                 [0.6, 0.1, -0.3],\n",
    "                 [0.7, -0.1, 0.6]])\n",
    "\n",
    "# 3 classes\n",
    "b_toy = np.array([[0.05, 0.1, -0.2]])\n",
    "\n",
    "# 4 examples x 5 features\n",
    "x_toy = np.array([[-1.0, -0.7, 0.3, 0.8, 0],\n",
    "                 [0.5, -0.2, 0.6, 0, -0.4],\n",
    "                 [0.1, 0, -0.4, -0.1, -0.2],\n",
    "                 [0.6, 0.7, 0.2, 0.1, 0.4]])\n",
    "\n",
    "# 3 classes one-hot\n",
    "y_true_toy = np.array([[0, 1, 0],\n",
    "                      [1, 0, 0],\n",
    "                      [0, 1, 0],\n",
    "                      [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.1\n",
    "#### (1 point)\n",
    "\n",
    "Using the definition of a neural network linear layer, complete the code in `linear` function. Recall that\n",
    "\n",
    "$$ \\mathbf{z}^{(i)} = W^{(i)}\\mathbf{x} + \\mathbf{b} $$\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "\n",
    "- __From now on, all the computations should be done in batches__, i.e. instead of a single example vector with the shape $[1\\times d]$ the network will take batches with the shape $[m\\times d]$.\n",
    "- Note that numpy arrays support [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html): during arithmetic operations, smaller arrays get automatically reshaped to make the shapes of operands compatible. You can take advantage of broadcasting starting from the cell below and in the subsequent cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    \"\"\"\n",
    "        Compute output of linear transformation given x, W, b\n",
    "        x: np.array, shape=[m, d] input\n",
    "        W: np.array, shape=[d, k] weights\n",
    "        b: np.array, shape=[1, k] biases\n",
    "\n",
    "        return np.array, shape=[m, k]\n",
    "    \"\"\"\n",
    "    # place your code here\n",
    "    return np.dot(x, W) + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented this function correctly, calling it on the following input\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "2 & 3\n",
    "\\end{bmatrix},\n",
    "x = \\begin{bmatrix}\n",
    "1 & 2\n",
    "\\end{bmatrix},\n",
    "b = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "would produce the result below:\n",
    "\n",
    "$$\n",
    "z = W \\times x^\\intercal + b^\\intercal\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6 & 9 \\\\\n",
    "\\end{bmatrix}^\\intercal\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\n",
      " [[ 0.99  0.27 -1.28]\n",
      " [ 0.11  0.29 -0.39]\n",
      " [-0.17 -0.2  -0.16]\n",
      " [-0.1   0.45  0.55]]\n"
     ]
    }
   ],
   "source": [
    "z_toy = linear(x_toy, W_toy, b_toy)\n",
    "print('Z:\\n', z_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "linear.z.toy": [
        [
         0.9900000000000001,
         0.27,
         -1.28
        ],
        [
         0.11000000000000003,
         0.29,
         -0.39
        ],
        [
         -0.16999999999999998,
         -0.19999999999999998,
         -0.16
        ],
        [
         -0.09999999999999994,
         0.45000000000000007,
         0.55
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit\n",
    "pm.record('linear.z.toy', z_toy.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.2 \n",
    "#### (1 point)\n",
    "Using the definition of the softmax function, complete the code in the `softmax` function. Recall that\n",
    "\n",
    "$$ \\text{softmax}\\left(\\mathbf{z}\\right)_i = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}} $$\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "\n",
    "- It is recommended to subtract the maximum from the input vector for each sample before the actual softmax implementation. This is done for computational stability and doesn't change the output of the softmax function due to its invariance to offsets property\n",
    "\n",
    "- You can verify your implementation by checking if the resulting probabilities add up to 1 for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "        Compute softmax values for each element (or score) in z\n",
    "        z: np.array, shape=[m, k]\n",
    "\n",
    "        return np.array, shape=[m, k]\n",
    "    \"\"\"\n",
    "    # place your code here\n",
    "    eps = 10e-10\n",
    "    z = z - np.max(z, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return np.exp(z) / (np.sum(np.exp(z), axis=1).reshape(-1, 1) + eps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented this function correctly, calling it on the following input\n",
    "\n",
    "$$\n",
    "z = \\begin{bmatrix}\n",
    "6 & 9 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "would produce the result similar to the one below:\n",
    "\n",
    "$$\n",
    "y = \\text{softmax}(z) = \\begin{bmatrix}\n",
    "0.05 & 0.95 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      " [[0.62890533 0.30612109 0.06497358]\n",
      " [0.35666543 0.42700605 0.21632852]\n",
      " [0.33551466 0.3255987  0.33888664]\n",
      " [0.21510956 0.37283929 0.41205115]]\n",
      "y sum by columns:\n",
      " [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_toy = softmax(z_toy)\n",
    "print('y:\\n', y_toy)\n",
    "print('y sum by columns:\\n', np.sum(y_toy, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "softmax.y.toy": [
        [
         0.628905330314555,
         0.30612108831586077,
         0.06497358074067891
        ],
        [
         0.35666543192495803,
         0.4270060479258997,
         0.21632851972213613
        ],
        [
         0.3355146593396666,
         0.3255987025962287,
         0.338886637725218
        ],
        [
         0.2151095601408122,
         0.3728392942861907,
         0.41205114516094593
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit\n",
    "pm.record('softmax.y.toy', y_toy.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.3\n",
    "#### (1 point)\n",
    "\n",
    "Compute the loss given the predicted probabilities and the target labels. Use the cross-entropy loss for calculation:\n",
    "$$ \\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N y_n\\log \\hat{y}_n$$\n",
    "where $\\hat{y}_n$ is the predicted probability of the correct class $y_n$ for the $n$-th training example, and $N$ is the total number of training examples. For one-hot-encoded labels representation, the equation becomes\n",
    "$$ \\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N \\sum_{j=1}^k y_n^j\\log \\hat{y}_n^j $$ where $y_n^j$ is one-hot encoded (1 at the position of the correct class and 0 elsewhere) and $\\hat{y}_n^k$ is a probability distribution over $k$ classes for the $n$-th training example.\n",
    "\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- To compute the probabilities, use the `softmax` function implemented above.\n",
    "- Use $e$ as a base for the logarithm.\n",
    "- It is also recommended to add a tiny `epsilon` to the predicted probabilites (inside the log of cross entropy). This helps to avoid taking logarithms of zero probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_true, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "        Compute the cross entropy loss function for model softmax output with respect to labels.\n",
    "\n",
    "        y: np.array, shape=[m, k] output predictions of softmax\n",
    "        y_true: np.array, shape=[m, k] one-hot encoded true labels\n",
    "        epsilon: float, small constant to add to input of log for computational stability\n",
    "\n",
    "        return: np.float, the value of the loss function\n",
    "    \"\"\"\n",
    "    # place your code here\n",
    "    return - 1/len(y) * np.sum( y_true * np.log(y + epsilon) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.4\n",
    "#### (1 point)\n",
    "\n",
    "Complete the code in `regularization` that adds a regularization term to the loss function. Your code also should compute the gradient of the loss with respect to the weights (see the formula to derive the mathematical expression for the derivative).\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "\n",
    "- Recall that regularization is computed as $\\lambda \\Vert W\\Vert ^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def regularization(params, lam=0.05):\n",
    "    \"\"\"\n",
    "        Compute gradients for squared weight regularization. A list of parameters\n",
    "        should be provided to this function (for example, [weight, bias] for a linear network). This regularization\n",
    "        should force parameters values to be small, keeping the overall model more simple. Here, a parameter is a grouping\n",
    "        of weights or biases in a single numpy array. Loss value and gradients are calculated together for simplicity.\n",
    "\n",
    "        params: list of np.arrays, each a parameter (matrix or vector) of the model to be regularized\n",
    "        lam: float, lambda coefficient multiplied by regularization term\n",
    "\n",
    "        Return: float value of regularization loss, list of np.array gradients, one per each param\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    value = 0\n",
    "    \n",
    "    # place your code here to update `value` and `gradients`\n",
    "    for param in params: \n",
    "        if param.shape[0] != 1:\n",
    "            value += lam*np.sum(np.square(param))\n",
    "            gradients.append(2*lam*param)\n",
    "        else:\n",
    "            gradients.append(np.zeros_like(param))\n",
    "\n",
    "    return value, gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 1.0558572651228835\n",
      "L2 regularization: 0.17849999999999996\n"
     ]
    }
   ],
   "source": [
    "xent_toy = cross_entropy(y_toy, y_true_toy)\n",
    "print('Cross entropy:', xent_toy)\n",
    "value, _ = regularization([W_toy, b_toy], lam=0.05)\n",
    "print('L2 regularization: %s' % value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "cross_entropy.xent.toy": 1.0558572651228835
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit\n",
    "pm.record('cross_entropy.xent.toy', xent_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now that the forward pass is complete, we can start backpropagating the error. Recall that our goal is to find $\\frac{\\partial \\mathcal{L}}{\\partial W}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}$ to perform the weight updates. We can expand these expressions using the chain rule as:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial W}$$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}}$$\n",
    "where $\\mathbf{z}$ is the linear layer output and $\\mathcal{L}$ is the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.5\n",
    "#### (3 points)\n",
    "Knowing that $\\text{softmax}\\left(\\mathbf{z}\\right)_i = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}}$, $\\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{z})$ and that $\\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N y_n\\log \\hat{y}_n$, __find the derivative__ $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$ and use it to complete the code in  `softmax_cross_entropy_backward`.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- In `softmax_cross_entropy_backward` the softmax function and the cross-entropy function are combined for the ease of differentiation.\n",
    "- When differentiating the loss function with respect to $z_j$ consider the cases $i=j$ and $i\\ne j$, where $i$ is the index of the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_backward(z, y_true):\n",
    "    \"\"\"\n",
    "        Compute gradients of loss with respect to linear outputs z and true labels (y_true).\n",
    "        This is the gradients produced by the softmax and cross entropy functions combined.\n",
    "\n",
    "        z: np.array, shape=[m, k]\n",
    "        y_true: np.array, shape=[m, k] one-hot encoded labels\n",
    "\n",
    "        return: gradient of loss with respect to z\n",
    "    \"\"\"\n",
    "    # place your code here \n",
    "    return 1./len(z) * (softmax(z) - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.6\n",
    "#### (3 points)\n",
    "\n",
    "Knowing that $\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}$, find the gradient of the loss with respect to each input of the linear function: $\\mathbf{x}$, $\\mathbf{W}$ and $\\mathbf{b}$. These would be $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}$, $\\frac{\\partial \\mathcal{L}}{\\partial W}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}$. For now, we only need the gradients of $\\mathbf{W}$ and $\\mathbf{b}$ for training our linear model. Later, we will need the gradient of the linear input $\\mathbf{x}$ in order to calculate gradients for multiple layers.\n",
    "\n",
    "\n",
    "Complete the code in `linear_backward` (given that you already computed $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$). This function computes the target gradients $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}$, $\\frac{\\partial \\mathcal{L}}{\\partial W}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}$.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- `z_grad` in the function below denotes $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$ that you computed in the previous function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(x, W, z_grad):\n",
    "    \"\"\"\n",
    "        Compute gradients for loss function with respect to inputs x, W, and b given z_grad (dL/dz).\n",
    "\n",
    "        x: np.array, shape=[m, d] input\n",
    "        W: np.array, shape=[d, k] weights\n",
    "        z_grad: np.array, shape=[m, k] gradient of loss with respect to output z\n",
    "\n",
    "        return: dL/dx: np.array, shape=[m, d],\n",
    "                dL/dW: np.array, shape=[d, k],\n",
    "                dL/db: np.array, shape=[1, k]\n",
    "    \"\"\"\n",
    "    dldx = np.dot(z_grad, W.transpose())\n",
    "    dldw = np.dot(x.transpose(), z_grad)\n",
    "    dldb = np.dot(np.ones((1, z_grad.shape[0])), z_grad)\n",
    "    return dldx, dldw, dldb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.7\n",
    "#### (3 points)\n",
    "\n",
    "Complete the code in the `update_weights` function using the gradients obtained in the previous step. Recall that, according to a single step of the gradient descent algorithm, the parameters are updated as follows:\n",
    "\n",
    "$$ W_{ij} \\leftarrow W_{ij} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}} $$\n",
    "\n",
    "$$ b_{i} \\leftarrow b_{i} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial b_{i}} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- The weights initialization is provided for you.\n",
    "- Inside the `update_weights` function you can call the previously implemented functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_params(x, y_true):\n",
    "    \"\"\"Given x and y pairs from the dataset, initialize parameters for the linear network.\n",
    "    \n",
    "    x: np.array, shape=[m, d]\n",
    "    y_true: np.array, shape=[m, k]\n",
    "    \n",
    "    Return: list of np.array parameters\"\"\"\n",
    "    n_features = x.shape[1]\n",
    "    n_classes = y_true.shape[1]\n",
    "\n",
    "    # Initialize W from normal distribution and b with zeros\n",
    "    W = np.random.randn(n_features, n_classes) / n_features\n",
    "    b = np.zeros((1, n_classes))\n",
    "    \n",
    "    return [W, b]\n",
    "\n",
    "def update_weights(params, x, y_true, alpha, lam=0.05, record_values=False):\n",
    "    \"\"\"\n",
    "        Update network weights as a step of gradient descent. This function should calculate gradients with respect\n",
    "        to the cross entropy loss, as well as the L2 weight regularization, then apply these gradients to the model\n",
    "        parameters.\n",
    "        \n",
    "        params: list of parameters to update\n",
    "        x: np.array, shape=[m, d] input\n",
    "        W: np.array, shape=[d, k] weights\n",
    "        b: np.array, shape=[1, k] biases\n",
    "        y_true: np.array, shape=[m, k] one-hot encoded labels\n",
    "        alpha: float, learning rate\n",
    "        lam: lambda regularization constant\n",
    "        record_values: used to write values\n",
    "        \n",
    "        return: computed loss for model output (cross entropy + regularization losses)\n",
    "    \"\"\"\n",
    "    W, b = params\n",
    "    \n",
    "    # place your code here\n",
    "    z = linear(x, W, b)\n",
    "    y = softmax(z)\n",
    "    loss = cross_entropy(y, y_true)\n",
    "    z_grad = softmax_cross_entropy_backward(z, y_true)\n",
    "    x_grad, W_grad, b_grad = linear_backward(x, W, z_grad)\n",
    "    \n",
    "    # calculate regularization\n",
    "    reg_loss, [W_reg_grad, b_reg_grad] = regularization(params, lam)\n",
    "    \n",
    "    W -= alpha * (W_grad + W_reg_grad)\n",
    "    b -= alpha * (b_grad + b_reg_grad)\n",
    "    \n",
    "    if record_values:\n",
    "        pm.record('z.toy', z.tolist())\n",
    "        pm.record('y.toy', y.tolist())\n",
    "        pm.record('loss.toy', loss)\n",
    "\n",
    "        pm.record('z_grad.toy', z_grad.tolist())\n",
    "        pm.record('x_grad.toy', x_grad.tolist())\n",
    "        pm.record('W_grad.toy', W_grad.tolist())\n",
    "        pm.record('b_grad.toy', b_grad.tolist())\n",
    "    \n",
    "    return loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:50: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "z.toy": [
        [
         0.9900000000000001,
         0.27,
         -1.28
        ],
        [
         0.11000000000000003,
         0.29,
         -0.39
        ],
        [
         -0.16999999999999998,
         -0.19999999999999998,
         -0.16
        ],
        [
         -0.09999999999999994,
         0.45000000000000007,
         0.55
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "y.toy": [
        [
         0.628905330314555,
         0.30612108831586077,
         0.06497358074067891
        ],
        [
         0.35666543192495803,
         0.4270060479258997,
         0.21632851972213613
        ],
        [
         0.3355146593396666,
         0.3255987025962287,
         0.338886637725218
        ],
        [
         0.2151095601408122,
         0.3728392942861907,
         0.41205114516094593
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "loss.toy": 1.0558572651228835
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "z_grad.toy": [
        [
         0.15722633257863874,
         -0.17346972792103482,
         0.016243395185169728
        ],
        [
         -0.1608336420187605,
         0.10675151198147492,
         0.05408212993053403
        ],
        [
         0.08387866483491666,
         -0.16860032435094283,
         0.0847216594313045
        ],
        [
         0.05377739003520305,
         0.09320982357154768,
         -0.14698721370976353
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "x_grad.toy": [
        [
         0.09160788248462307,
         -0.23908817799932786,
         -0.10895485532389444,
         0.07211580819952884,
         0.13715144270825244
        ],
        [
         -0.037151117032927554,
         0.23043403697798304,
         0.04782626819904959,
         -0.10204967299226901,
         -0.09080942265295941
        ],
        [
         0.10971665998791842,
         -0.14276232918946888,
         -0.1265766924484292,
         0.008050668636464359,
         0.12640809347831863
        ],
        [
         -0.09070107591930546,
         -0.05126864237265956,
         0.10002205824555638,
         0.08568358049120565,
         -0.059869137558370755
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "W_grad.toy": [
        [
         -0.1969888530834055,
         0.2659113456196066,
         -0.06892249250263038
        ],
        [
         -0.04024753137665288,
         0.16532538364851274,
         -0.12507785221256007
        ],
        [
         -0.07212827336459071,
         0.09809208326726118,
         -0.025963810000603174
        ],
        [
         0.12277093858293964,
         -0.11259476754457881,
         -0.01017617116597102
        ],
        [
         0.06906867985460209,
         0.028303389506217674,
         -0.09737206934237994
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:57: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "b_grad.toy": [
        [
         0.13404874542999795,
         -0.14210871671895503,
         0.008059970837244718
        ]
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit\n",
    "loss = update_weights([W_toy, b_toy], x_toy, y_true_toy, 0.1, record_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2343572651228834\n"
     ]
    }
   ],
   "source": [
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.8\n",
    "#### (2 points)\n",
    "Recall that training of a neural network is done in iterations such that after every iteration the network produces the predictions, computes the value of the loss function based on the predictions and the ground-truth labels, calculates the loss gradients with respect to every weight, and then uses the derived gradients to update the weights.\n",
    "\n",
    "Keeping this logic in mind, complete the code in the `train` function\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- Append the mean loss of each epoch to a list to be able to plot the learning curve after the training finishes.\n",
    "- The current implementation takes the `init_func` and `update_func` as the arguments, which are going to be different for a single-layer network and a multi-layer network. This allows us to unify the training loop for both architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(init_func, update_func, x, y_true, batch_size=50, num_epoch=1, learning_rate=1e-3, lam=0.001, plot_loss=True, plot_per_epoch=True):\n",
    "    \"\"\"\n",
    "        Train a neural network.\n",
    "        \n",
    "        init_func: function mapping (x, y_true) --> params\n",
    "        update_func: function mapping (list params, np.array x, np.array y_true, float alpha, float lam) --> float loss, while updating parameters\n",
    "        x_train: np.array, shape=[m, d]\n",
    "        y_train: np.array, shape=[m, k] one-hot encoding of labels\n",
    "        batch_size: int, number of examples in a batch\n",
    "        num_epoch: int, number of training iterations\n",
    "        learning_rate: float, learning rate\n",
    "        lam: float, regularization parameter\n",
    "        plot_loss: bool, optionally plot the learning curve\n",
    "        \n",
    "        return: list of losses per batch over all epochs, learned list of parameters\n",
    "    \"\"\"\n",
    "\n",
    "    losses = []\n",
    "    batch_losses = []\n",
    "    \n",
    "    n_batches = x.shape[0] // batch_size\n",
    "    params = init_func(x, y_true)\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        for iter_num, (x_batch, y_batch) in enumerate(zip(np.array_split(x, n_batches), np.array_split(y_true, n_batches))):\n",
    "            # place your code here\n",
    "            loss = update_func(params, x_batch, y_batch, learning_rate, lam)\n",
    "            batch_losses.append(loss)\n",
    "            \n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        losses.append(epoch_loss)\n",
    "        batch_losses = []\n",
    "        print(f'Epoch: {epoch:<5} Loss: {epoch_loss:.3f}')\n",
    "        \n",
    "            \n",
    "    # draw learning curve \n",
    "    if plot_loss:\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Loss\")\n",
    "        if plot_per_epoch:\n",
    "            plt.xlabel(\"epochs\")\n",
    "        else:\n",
    "            plt.xlabel(\"batches\")\n",
    "        plt.show()\n",
    "        \n",
    "    return losses, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Loss: 1.287\n",
      "Epoch: 1     Loss: 0.700\n",
      "Epoch: 2     Loss: 0.421\n",
      "Epoch: 3     Loss: 0.287\n",
      "Epoch: 4     Loss: 0.220\n",
      "Epoch: 5     Loss: 0.182\n",
      "Epoch: 6     Loss: 0.158\n",
      "Epoch: 7     Loss: 0.141\n",
      "Epoch: 8     Loss: 0.129\n",
      "Epoch: 9     Loss: 0.120\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "losses_toy, params = train(init_params, update_weights, x_toy, y_true_toy, batch_size=1, num_epoch=10, learning_rate=1)\n",
    "W_learned, b_learned = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "losses.toy": [
        1.286616132400821,
        0.7001725049303289,
        0.42105714449607967,
        0.2868850555210989,
        0.22038574537190597,
        0.18210995574078861,
        0.15765945302008919,
        0.14089711419070297,
        0.1288120399729253,
        0.11976640643357052
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit\n",
    "pm.record('losses.toy', losses_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now that we have the `train` function, we can train our network on the 20 Newsgroups dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.9\n",
    "#### (1 point)\n",
    "\n",
    "Now we can test the implemented functions on our real dataset.\n",
    "\n",
    "In the cell below, create the `X` and `y_true` matrices. During construction of the vocabulary, discard all tokens that appear less than 10 times (set the `min_count` argument to `10`)\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- You can use the functions `tokenize_doc`, `build_vocab`, `doc_to_multihot`, and `labels_to_onehot`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create X and y_true here\n",
    "vocab = build_vocab([tokenize_doc(d) for d in dataset.data], min_count=10)\n",
    "x = np.array([doc_to_multihot(tokenize_doc(doc), vocab) for doc in dataset.data])\n",
    "y_true = np.array(labels_to_onehot(dataset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2247, 5634)\n",
      "y_true: (2247, 4)\n"
     ]
    }
   ],
   "source": [
    "print('X:', x.shape)\n",
    "print('y_true:', y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "x.shape": [
        2247,
        5634
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "x.max": 1162
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "x.min": 12
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "x.mean": 121.46372941700045
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "y_true.shape": [
        2247,
        4
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "y_true.max": 1
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "y_true.min": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "y_true.mean": 0.25
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('x.shape', x.shape)\n",
    "pm.record('x.max', x.sum(axis=1).max())\n",
    "pm.record('x.min', x.sum(axis=1).min())\n",
    "pm.record('x.mean', x.sum(axis=1).mean())\n",
    "pm.record('y_true.shape', y_true.shape)\n",
    "pm.record('y_true.max', y_true.max())\n",
    "pm.record('y_true.min', y_true.min())\n",
    "pm.record('y_true.mean', y_true.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 3.10 \n",
    "#### (2 points)\n",
    "\n",
    "Using the implemented `train` function, now train a neural network on the Newsgroups dataset. Once the network is trained, complete the code in the `make_prediction` function that makes predictions for given inputs.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- Recall that $\\hat{y}_{pred} = \\underset{j \\in 1..k}{\\text{argmax}}\\left[\\text{softmax}\\left(\\mathbf{z}\\right)\\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Loss: 1.155\n",
      "Epoch: 1     Loss: 0.820\n",
      "Epoch: 2     Loss: 0.649\n",
      "Epoch: 3     Loss: 0.545\n",
      "Epoch: 4     Loss: 0.476\n",
      "Epoch: 5     Loss: 0.426\n",
      "Epoch: 6     Loss: 0.388\n",
      "Epoch: 7     Loss: 0.358\n",
      "Epoch: 8     Loss: 0.334\n",
      "Epoch: 9     Loss: 0.314\n",
      "Epoch: 10    Loss: 0.297\n",
      "Epoch: 11    Loss: 0.283\n",
      "Epoch: 12    Loss: 0.270\n",
      "Epoch: 13    Loss: 0.260\n",
      "Epoch: 14    Loss: 0.250\n",
      "Epoch: 15    Loss: 0.241\n",
      "Epoch: 16    Loss: 0.234\n",
      "Epoch: 17    Loss: 0.227\n",
      "Epoch: 18    Loss: 0.221\n",
      "Epoch: 19    Loss: 0.215\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAETCAYAAAArjI32AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8XNV99/HPLNJoJI2ksbVY8oq3Y2Nsg22wHZawhyUkNKUJCSSBJgGatmnDk6aBpy1Jn7RZmjTN0hSyUEIgpWnASSABQsO+GAeMjddjy7a8SF4kW/u+PX/ckRjLkiVbmrlzZ77v10svzdx7Z/TT1eg7Z86951xff38/IiLiXX63CxARkfFRkIuIeJyCXETE4xTkIiIepyAXEfE4BbmIiMcpyCWtGWNmGWNa3K5DJJEU5CIiHhd0uwARNxhjCoF/B84G+oEngbuttT3GmC8BfwR0AUeBW6y1B0da7sovIBJHLXLJVN/BCePFwApgKfA5Y8x04K+Bc621K4DfAStHWu5K5SJDKMglU10NfM9a22+t7QTujS2rBjYC640x3wA2WGt/eZLlIq5TkEum8uN0qcTfz7LW9gHvBm7BabF/yxjz9ZGWJ7VikREoyCVTPQ38hTHGZ4wJAbcBzxhjlgKbgW3W2q8A3wLOHWm5S7WLHEcHOyUT5A1zCuK1wO3AJiAbeAr4J2ttlzHm58Absce0A5+x1m4cbnnyfgWRkfk0ja2IiLepa0VExOMU5CIiHjemPnJjzErga9bai4cs/zDOubW9wNvAp2NH90VEJElGbZEbYz4P/AjIGbI8DHwZuMRa+y6gEHhvIooUEZGRjaVFvgv4APDTIcs7gXdZa9vinqtjtCerrW0+7aOr0Wgu9fVto2/oklSvD1K/RtU3PqpvfFK5vpKSiG+kdWM6a8UYMwt4xFq7aoT1fwlcA1xjrT3pE/b09PYHg4FRf6aIiBxnxCAf13nkxhg/8HVgPvDHo4U4MK53u5KSCLW1zaf9+ERL9fog9WtUfeOj+sYnlesrKYmMuG68A4Luw+liuV4HOUVE3HHKQW6M+QiQD7wBfAJ4CXjWGAPwbWvtmgmtUERETmpMQW6trQJWxW7/LG6VzkMXEXGZglhExOMU5CIiHqcgFxHxOM8E+bGmDh54Ygud3b1ulyIiklI8E+Rb9hzj0ecqeWP7EbdLERFJKZ4J8hllzsnwldWNLlciIpJaPBPk00rzyMkOKMhFRIbwTJAH/H7mz4hSU9tKW0e32+WIiKQMzwQ5wMJZk+gHdtc0uV2KiEjK8FSQL5g1CYCdB9S9IiIywFtBPjMK6ICniEg8TwV5fm42FcV57D7YRG+fJlsUEQGPBTnA3KkFdHb1Ul3b6nYpIiIpwXNBPmdqIaDuFRGRAZ4L8nnTigAFuYjIAM8FeVk0TH44i0qduSIiAngwyH0+H3OnFlLX2EF9c6fb5YiIuM5zQQ4wZ2oBALvUvSIi4s0gn6sDniIigzwZ5LPKCwj4fWqRi4jg0SAPZQWYURah6lAz3T260ISIZDZPBjk43Su9ff3sOdjsdikiIq7ybpBPc/rJ1b0iIpnOs0E+p8I5c0UHPEUk03k2yCcV5DC5IERldSP9/f1ulyMi4hrPBjk48640t3VzpKHd7VJERFzj6SAfnHdFw/VFJIN5Osg1MEhExONBPq00j+wsv4JcRDKap4M84Pczu7yAmtpW2jq63S5HRMQVng5ycM4n7wd21zS5XYqIiCu8H+TqJxeRDOf5IJ9d4QT5Tp25IiIZyvNBnh/OoqI4j90Hm+jt63O7HBGRpBtTkBtjVhpjnh9m+XXGmD8YY14zxnxqwqsbo7lTC+js6qW6ttWtEkREXDNqkBtjPg/8CMgZsjwL+BZwJfBu4DZjzJREFDmaOeonF5EMNpYW+S7gA8MsXwhUWmvrrbVdwMvAhRNZ3FjpgKeIZLLgaBtYax81xswaZlUBEJ+czUDhaM8XjeYSDAbGXOBQJSWRE5YVF+cTyc1iz8HmYdcnk9s/fyxSvUbVNz6qb3xSvb7hjBrkJ9EExP/GEaBhtAfV17ed9g8sKYlQWzv8hSRmlxewcddRdu6poyg/dNo/YzxOVl+qSPUaVd/4qL7xSeX6TvYGM56zVrYB84wxk4wx2cBFwGvjeL5xGbjQhCbQEpFMc8pBboz5iDHmNmttN3An8DROgN9vra2e6ALHSv3kIpKpxtS1Yq2tAlbFbv8sbvnjwOMJqewUzSovIOD36dJvIpJxPD8gaEAoK8CMsnyqDjXT3dPrdjkiIkmTNkEOzvnkvX39VB1KzYMVIiKJkFZBPthPrgOeIpJB0jPI1U8uIhkkrYJ8UkEOkwtCVFY30t/f73Y5IiJJkVZBDk4/eXNbN0ca2t0uRUQkKdIuyNVPLiKZJv2CPDbCU+eTi0imSLsgn1aST3aWn50KchHJEGkX5MGAn9nlBdTUttLW0e12OSIiCZd2QQ5O90o/sLumye1SREQSLj2DXOeTi0gGScsgn12hIBeRzJGWQZ4fzqJ8ci67apro69PAIBFJb2kZ5OB0r3R29XKgtsXtUkREEiqtgxzUvSIi6S99g3yaglxEMkPaBvmUSbnk5QQ1VF9E0l7aBrnP52Pu1ELqGjtoaOl0uxwRkYRJ2yAHzbsiIpkhvYM8dsBzp7pXRCSNpXWQzyovwO/zqUUuImktrYM8lBVgRlk+VYea6e7pdbscEZGESOsgB6efvLevn6pDzW6XIiKSEOkf5BoYJCJpLnOCXAc8RSRNpX2QTyrIYVJBiMrqRvr7NYGWiKSftA9ycFrlzW3dHGlod7sUEZEJlxFBPkfdKyKSxjIiyOdphKeIpLGMCPJpJflkZ/l15oqIpKWMCPJgwM/s8gKqa1tp6+h2uxwRkQmVEUEOTj95P2D3NbhdiojIhMqYIF9hSgF4dv0BlysREZlYwdE2MMb4ge8DS4FO4JPW2sq49Z8DPgz0Af9srV2ToFrHZeaUCGZ6EVuq6jlQ28K0kny3SxIRmRBjaZFfD+RYa1cDXwC+ObDCGFMEfAZYDVwJ/FsiipwoV543HYDf/WG/y5WIiEycsQT5BcBTANbatcCKuHWtwF4gL/bVN9EFTqSlc4spjYZZu+UQja1dbpcjIjIhRu1aAQqA+PP2eo0xQWttT+z+fmArEAC+MtqTRaO5BIOBUy50QElJ5LQfC/CBS+Zx72Nv8/r2Wm66asG4nms4460vGVK9RtU3PqpvfFK9vuGMJcibgPjfzB8X4lcD5cAZsftPG2NesdauG+nJ6uvbTqtQcHZwbe34pqNdOitKXk6QJ17ezcVLppCddfpvKkNNRH2Jluo1qr7xUX3jk8r1newNZixdK68A1wAYY1YBm+LW1QPtQKe1tgNoAIpOu9IkCGUHePfZU2lp72bt1sNulyMiMm5jCfI1QIcx5lXgW8BnjTF3GmPeZ619CfgDsNYY8xqwA3gmceVOjMuWTyPg9/H0un2aEVFEPG/UrhVrbR9wx5DF2+PW3wPcM8F1JVQ0EuK8haW8tuUwm/ccY/HsyW6XJCJy2jJmQNBQV547A4DfrdvnciUiIuOTsUE+dICQiIhXZWyQgwYIiUh6yOgg1wAhEUkHGR3kfp+PK8+dTk9vP89pMi0R8aiMDnKA888qJy8nyLPrq+nq7nW7HBGRU5bxQa4BQiLidRkf5KABQiLibQpy3hkgdPBoG5v3HHO7HBGRU6Igj9EAIRHxKgV5zHEDhI5ogJCIeIeCPM7gAKE3NEBIRLxDQR5HA4RExIsU5HE0QEhEvEhBPoQGCImI1yjIh9AAIRHxGgX5MDRASES8REE+DA0QEhEvUZCPQAOERMQrFOQjmDklwoIZGiAkIqlPQX4SV5yrAUIikvoU5CehAUIi4gUK8pPQACER8QIF+Sg0QEhEUp2CfBSh7AAXn6MBQiKSuhTkY3DpMg0QEpHUpSAfg/gBQuu2HXG7HBGR4yjIx+i6888gO8vPg09bahva3S5HRGSQgnyMpkzK5eYrDO2dPdz36y309Pa5XZKICKAgPyXnL57CqkVl7K5pYs2Lu90uR0QEUJCfEp/Px0evNJRFwzz5+j427T7qdkkiIgryUxUOBbnj/WcRDPj40RNbqW/udLskEclwCvLTMHNKhD+5ZC7Nbd388PEt9PXplEQRcY+C/DRdvnwa58wrZvu+Bn7zWpXb5YhIBguOtoExxg98H1gKdAKftNZWxq2/Grgndnc98OfW2rRvovp8Pm69ZiF7/3Mdv3x5D2ZGlJKSiNtliUgGGkuL/Hogx1q7GvgC8M2BFcaYCPAvwHuttauAKqA4AXWmpPxwFrddtwgfPu779RaaNEOiiLhgLEF+AfAUgLV2LbAibt27gE3AN40xLwGHrbW1E15lCps/vYj3X3gG9c2d/Nsj6zWEX0SSbtSuFaAAaIy732uMCVpre3Ba35cAZwMtwEvGmNestTtGerJoNJdgMHDaBadi98XHrzuL3Qeb+MPWw5w9r4T3XTTH7ZJOKhX3YTzVNz6qb3xSvb7hjCXIm4D438wfC3GAo8AfrLWHAIwxL+KE+ohBXl/fdpqlOju4trb5tB+fSLe8x/Clg83c//gWyqM5zJpS4HZJw0rlfQiqb7xU3/ikcn0ne4MZS9fKK8A1AMaYVThdKQPeBM4yxhQbY4LAKmDr6ZfqXYX5IT774WX09vVz7y+30N7ZM/qDREQmwFiCfA3QYYx5FfgW8FljzJ3GmPfF+sPvAp4GXgces9ZuTly5qW3ZglKuXjWDIw3tPPi0VX+5iCTFqF0r1to+4I4hi7fHrX8EeGSC6/KsP7pwNjv2NfD61sOcOTPKhUsr3C5JRNKcBgRNsGDAz+3vW0RuKMjDz+yguq7V7ZJEJM0pyBOguCjMrdcsoKunj3t/tVnX+hSRhFKQJ8hyU8oly6ZSXdvKf/1+p9vliEgaU5An0I2XzmVaST4vbKhh3TZduFlEEkNBnkBZwQB/dv0isrP8/OSp7RzRJeJEJAEU5AlWPjmPj15paO/s5b5fbdYl4kRkwinIk+D8xeWsXjSFPQeb+e6jm+js0sFPEZk4CvIk+dhVhsWzJ7Np91H+5ZG3aGnvdrskEUkTCvIkCWUF+Ms/XszqRVPYXdPEVx56k6ONHW6XJSJpQEGeRMGAn0+8dyHvOW86B4+28c8PvakBQyIybgryJPP7fHzo0nn8ySVzqG/u5KsPvUlldePoDxQRGYGC3CVXr5zJJ65dSHtnL9/4r7fYWFnndkki4lEKchedv7icv/jjxQB899FNvLLpoMsViYgXKchddvbcYj534zmEQwF+/JttPPX6PrdLEhGPUZCngLnTCvnCTcuIRkL8/LlKfv5sJX2ay1xExkhBniKmluRz983LmTIpl6fW7eP+32zTKFARGRMFeQqZXJjDXTcvY3ZFAa9uPsT3HtMoUBEZnYI8xURys/mbG8/hrDMm8fauo3xDo0BFZBQK8hQUyg7wmRuWsGpRGbtio0CPNWkUqIgMT0GeooIBP59875lcea4zCvSffvomNRoFKiLDUJCnMGcU6FxuuNgZBfqVh95kgwYOicgQCvIU5/P5uGbVTG69ZgEdXb185xdv84PHt6jfXEQGBd0uQMbmwiUVnFFewH/+dhtrtxxm655j3HylYcWCUrdLExGXqUXuIdNK8rn7o8v54CVzae/q5fu/3My/r9lEY2uX26WJiIvUIveYgN/PVStncPa8Yv7zt9t409ayfW89H7l8PqsWleHz+dwuUUSSTC1yj5oyKZe/vWkZN10xn57efn74xFa+/Yu3dZqiSAZSkHuY3+fjsuXT+MdPnMfCmVHe3nWUv//x67y4sYZ+zdUikjEU5GmgpCjM5248m1uuXgDAA09u5xuPbKC2od3lykQkGRTkacLn83HR0gr+3ydWsmTOZLbtrecffryO3795QDMpiqQ5BXmamVSQw1/dsIRPXXcmwYCPh5/ZwdceXs+hY21ulyYiCaIgT0M+n4/Vi6bw5U+tYoUpYeeBRu65fx1Pvr5XU+OKpCEFeRorzMvm03+0mE9ffxbh7AD/89wubv/q73lxY40CXSSN6DzyDLBiQSkLZkb51Ut7ePHtGh54cjtPvFrFtatncv7icoIBvZ+LeJn+gzNEfjiLm66czw/vvpzLlk+joaWLnzxlueu+tTy/oVotdBEPG7VFbozxA98HlgKdwCettZXDbPMb4FfW2nsTUahMjMmFYW66Yj7XrJrJk6/v5YUNNTz4lOU3r1Zx7epZXLBELXQRrxnLf+z1QI61djXwBeCbw2zzZWDSRBYmiRWNhPjI5fP52h2ruWLFdJraunnwacsX7nuN596qprtHLXQRrxhLkF8APAVgrV0LrIhfaYy5AegDnpzw6iThivJDfPjyeXz9jtVcee50Wtq6+elAoK8/oEAX8QDfaEO5jTE/Ah611j4Zu78PmG2t7THGnAX8I3AD8A/AodG6Vnp6evuDwcCEFC8Tr765g8eeq+S3r1bR1d1LcWEON1w6jytWziQ7S383EReNOCPeWM5aaQIicff91tqe2O2PAVOBZ4FZQJcxpspa+9RIT1Zff/oDU0pKItTWNp/24xMt1euDsdX4vtUzefeScp5+fR/PvnWAe9ds4r//dwfXrJrJBYvLCWUnLtBTfR+qvvFRfaevpCQy4rqxBPkrwHXAz40xq4BNAyustZ8fuG2M+SJOi3zEEBfvKMzL5oOXzuWqlTN4at0+nl1/gIef2cGjL+xi5ZllXLS0gllTIpo2VyQFjCXI1wBXGGNexWna32qMuROotNb+OqHViesK8rL54CVOoD+3vpqX3q7hhQ3O1/TSfC5aWsGqRWXk5WS5XapIxhq1j3yi1dY2n/YPTOWPPZD69cH4a+zr62dL1TFe3FjDhp119Pb1Ewz4WbGghIuWVGBmFI2rlZ7q+1D1jY/qO30lJZFx9ZGLDPL7fSyePZnFsyfT2NrFq5sP8uLGg6zdcpi1Ww5TGg1z0dIKzj9rCoX5IbfLFckICnI5bYV52Vy9ciZXnTeDnQcaeWFDDW/YI/zi+V089sJuls6dzEVLK1g8ezJ+v/rSRRJFQS7j5vP5mD+9iPnTi7jpinms3XqYFzfU8NbOOt7aWUc0EuKCxeVcuKSc4qKw2+WKpB0FuUyo3JwsLl02jUuXTaPqUFOs2+UQj79axeOvVjGnooBlpoRl80soi+a6Xa5IWlCQS8LMmlLArCkFfOiSubxhj/DKpoPY/Q3sqmnif57bxdSSPJbNc0J9Rlm+TmUUOU0Kckm4UHaA8xeXc/7icprauti4s471O2rZUlU/2FIvLszhnHklXLZyJsV5WepTFzkFCnJJqoLcbC5cWsGFSyto7+xh855jrN9Ry8bKOp55Yz/PvLGfSG4W58wrZtn8EhbOnERWULMxipyMglxcEw4FOXdBKecuKKW7p49te+vZtr+B196u4cWNzmmNOdkBlsyZzLL5JSyePZlwSC9ZkaH0XyEpISvoZ8mcyVy2ahZ/ctFsKqsbWb+jlvU7alm37Qjrth0hGPAxd2ohC2dN4syZUWaVRwj41VoXUZBLyvH73zmd8UOXzmX/kRbW76hlw846tu9rYPu+BtYA4VAAMz3KwllRzpwZpaI4TwdMJSMpyCWl+Xw+ZpRFmFEW4foLZ9PU1sX2vfVON0xVPRsq69hQWQc4A5QWzoqycGaUM2dOYnJhjsvViySHglw8pSA3m/MWlnHewjIA6hrb2VblBPvWvfWDUwUAlEbDnBnrhlkwM0p+WBN7SXpSkIunFReGuXBpmAuXVtDf3091XetgsG/fV8/zb1Xz/FvV+IDpZfnMm1bEnKkFzKkopLgwR10xkhYU5JI2fD4f00rymVaSzxXnTqe3r4+qg81s3VvPtqpjVFY3su9wC79/09m+IC+bORUFzJlayJwKZ/BSIi+aIZIoCnJJWwG/3wnpqYVc965ZdPf0svdwC7urG6msaWJXdePgfDAAfp+PaaV5zJlayNyKQmZPLaC0KKxWu6Q8BblkjKxggLlTC5k7tZArY8uONXWwu6aJXTWN7KpuoupQM/sOt/Dc+moA8sNZx7Xaw/k6gCqpR0EuGW1SQQ6TCnJYsaAUgJ7ePvYdbmFXdeNguG/cdZSNu47GHrGB4sIc50ya0nyml+UzsyxCNBJSy11coyAXiRMM+JldUcDsigKuYDoADS2d7KpuYndNI4fq26k80DA4WGlAXk4wdppkPjNKI0wvy6d8cq4GLElSKMhFRlGUH2K5KWG5KaGkJMKRI000tHSx/4jTDbPvcDP7jrQ457bvrR98XDDgZ1pJHjPK8pleGmFmWYSK4jxyc/RvJxNLryiRU+Tz+YhGQkQjIZbMKR5c3t7Zw/4jLew/Egv3wy0cqG2h6lAzcHBwu6L8bCqK86iYnOd8L86jfHIukdxsF34bSQcKcpEJEg4FB6cWGNDT28eho23sPdzM/iMt1Bxt5WBdK1ur6tlaVX/c4yO5WSeEe0VxHoV52ep/l5NSkIskUDDgZ1ppPtNK849b3t7Zw6FjbdTUtcbC3bm9Y38Ddn/DcdvmhoKDwT5lUi6l0VzKomFKomFCWTrvXRTkIq4Ih4KcUV7AGeUFxy3v6u51Av5oKzV1bRyMBf2eg01UVjee8DzRSIiyaJjSaJiyaC5zZ04iHPAp5DOMglwkhWRnBQYnCYvX09vH4fp2jhxrc77Xv/PdxmaEdOwafEw0EqK0KEzZpPBgK764MExJUQ65OZp3Jp0oyEU8IBjwM7U4j6nFeSes6+7p5UhDB0eOtdHa3cfu/fWDIT9cVw04nwiKC3MoLsxhcmEOJYXhwdvFhWGdWeMx+muJeFxWMDAY8iUlEWprmwfXDYZ8fRuHj7VztLGDusZ26ho7OFLfzv4jLcM+Z+5A0BfFB3wOkwtyiEZC5IezdAA2hSjIRdJYfMgP1d/fT0t7N3WNHRxt7KA2FvBO2HdwqL6NfSMEfVbQTzQSYlIkxKRYuA9+j93Oywkq7JNEQS6SoXw+H5HcbCK52SccdAUn6Jvbu6lrcFrxRxs7ONbUybHmDo41d1Lf1MH2+vYRnz87y080kuMEeyREtCCH6eUFBPv7KcwPUZSfTUFeNsGARr+Ol4JcRIbl8/koyM2mIDeb2RUnBj04XTf1LV3UNw0N+djtpk4OH2sb+WfgnD9flB8aDPfC/BDR2PciBf6YKMhF5LRlBQOUFoUpLQqPuE13Ty/Hmjs51tRJn9/P/ppGGlo6Y19dNLZ0cri+fcRuHHACPz83i8I8J9QL8pw3mKH3ndtZGTfHjYJcRBIqKxigLJpLWTTXORg7vXDY7do7e2ho6aSxpWsw5BtaOmls7aKhuZOG1i6ONnVwoLZ11J+ZHz4x9AvysmJdSXHfw9mEQwHP9+UryEUkJYRDQcKhIOWTTzwwG6+7p5fG1i6aWrtpau2iqa0rdj/uq815E6iuGz30A34f+bFQn1yUQyjoJxIeCHwn9PPDzu383GzycoIp182jIBcRT8kKBigudAY3jaant28w2Jtau2hu6459Obdb2t+5fbSpnQO1I3fvxMvJDpAfziIvnEX+wFdOFnnh4PHLBrbJyUpoy19BLiJpKxjwD148ZCyKonns2XfMCfdYyLcMhH+7E/ytse8t7d0crGulq6dvTM8d8Pv48OXzuHTZtPH8SsMaNciNMX7g+8BSoBP4pLW2Mm79Z4EbY3d/a6390oRXKSKSBAPnx0cjoTE/pqu7dzDYWzt6jgv6+OBv7+whmj/25z0VY2mRXw/kWGtXG2NWAd8E3g9gjJkN3ASsBPqBl4wxa6y1byekWhGRFJOdFWBSVmDMrf5EGEuP/QXAUwDW2rXAirh1+4GrrLW91to+IAvomPAqRURkRL7+/v6TbmCM+RHwqLX2ydj9fcBsa21P3DY+4F+AiLX29pM9X09Pb38wqOk1RURO0YhHSsfStdIExM+p6R8S4jnA/UAz8OnRnqy+fuRRXqMZOiFQqkn1+iD1a1R946P6xieV6yspiYy4bixdK68A1wDE+sg3DayItcR/BWy01t5ure0dX6kiInKqxtIiXwNcYYx5Fadpf6sx5k6gEggA7wZCxpirY9vfZa19LSHViojICUYN8thBzDuGLN4ed9u9Q7UiIjKmrhUREUlhCnIREY8b9fRDERFJbWqRi4h4nIJcRMTjFOQiIh6nIBcR8TgFuYiIxynIRUQ8TkEuIuJxKXmptzFclehTwO1AD/Bla+0TSa4vC2fGx1lAKFbDr+PW3wl8AqiNLbrdWmuTXONbQGPs7h5r7a1x69zef7cAt8Tu5gBnA1OstQ2x9d8BzseZURPg/dbaRpLAGLMS+Jq19mJjzFzgAZyLpmwG/jw2ZcXAtmHgIaA0VuvHrbW1Jz5rwuo7G/gu0Ivzf/Ixa+3hIduP+DpIQn3LgMeBnbHV/2Gt/e+4bd3ef48AU2KrZgFrrbU3xm3rAw7E1f+atfauRNZ3ulIyyDn5VYmmAJ/BucBFDvCyMeYZa21nEuu7GThqrf2oMWYy8Bbw67j1y3D+qd5MYk2DYlMLY629eJh1ru8/a+0DOAGJMebfgfsHQjxmGfAea21dsmqK1fJ54KPAwKXX/xX4O2vt88aYe3Feg2viHvJnwCZr7ReNMTcCfwf8VRLr+zbwl9baDcaY24G/Be6M237E10GS6lsG/Ku19psjPMTV/TcQ2saYKPAc8NkhD5kDrLfWXpeomiZKqnatnOyqROcBr1hrO2OttEpgSZLr+x/g7+Pu9wxZvxy4yxjzsjHGjXfwpUCuMeZ3xphnY2+GA1Jh/wFgjFkBLLLW/iBumR+YB/zAGPOKMeZPk1jSLuADcfeXAy/Ebj8JXD5k+8HX6QjrJ9rQ+m601m6I3Q5y4tW5TvY6SEZ9y4FrjTEvGmN+bIwZOqG22/tvwJeA71prDw5ZvhyYaox5zhjzW2OMSXB9py1Vg7yAdz4OAvQaY4IjrGsGCpNVGIC1tsVa2xx7Yf4CpyUR7xGcGSMvBS4wxrw3mfUBbcA3gPfE6ng4lfZfnLtx/oni5eF0F9wMXAV82hiTlDcaa+2jQHfcIp+1dmAOi+H2U/y+TPh+HFrfQPAYY962anvoAAAEZUlEQVQF/AXwrSEPOdnrIOH1AeuAv7HWXgTsBu4Z8hBX9x+AMaYUuIzYJ8QhDgJfsdZeAvwzTjdQSkrVID/ZVYmGrosA8R/Lk8IYMx3n49hPrbU/i1vuA/7NWltnre0CfgOck+TydgAPWWv7rbU7gKNAeWxdquy/ImCBtfa5IavagG9ba9ustc3AszgtSzf0xd0ebj/F70u39uOHgHuBa4fpXz7Z6yAZ1sR1L67hxP8D1/cfcAPwsxEuivMGzoVzsNa+jNM6H/Fya25K1SAf8apEOO/yFxpjcowxhcBCnANRSWOMKQN+B/yttfb+IasLgM3GmPzYH/1SINl95X+Kc1wBY0xFrKaBj42u77+Yi4D/HWb5fJx++0DsoPIFwPqkVvaOt4wxF8duXw28NGT94Ot0hPUJZYy5GaclfrG1dvcwm5zsdZAMTxtjzovdvowT/w9c3X8xl+N06wznHuCvAYwxS4F9cZ/QUkqqHuwc8apE1tpfx85qeAnnjej/WmuH9g0m2t1AFPh7Y8xAX/kPgTxr7Q+MMXfjtNY7gd9ba3+b5Pp+DDxgjHkZ54yLPwU+Y4xJlf0HYHA+bjt3jv/7PgysxfkY/KC1dosL9QH8H+CHxphsYBtONxrGmN8B7wX+A/hJbD93AR9JVmHGmADwHWAf8Fis+/YFa+09xpgHcbr7TngdxF9vNwn+DPieMaYLOATcFqvd9f0X57jXIRxX31eBh4wx1+IcB7sl6dWNkaaxFRHxuFTtWhERkTFSkIuIeJyCXETE4xTkIiIepyAXEfE4BbnIKIwxtxhjHnC7DpGRKMhFRDxO55FL2jDGfAH4IBAAnsYZcPIrYDuwCNgL3GytPRab/+bLOI2Z3ThTDR82xlyOMxrSH9v+IzgTLX0SZ1DIDJxBXp8yxkwDHsaZH6YP+ExskjeRpFKLXNKCMeYqnNnqzsWZ02MqcBOwGPi+tXYRzujML8YmSroPuN5auwRnqPj3jDEhnGD+uLV2Mc7UEB+P/YgZOIG+ELjaGLMIZ875J6y1K4B/wJlOQCTpUnWIvsipuhxYyTvzeYRxGio7rLXPx5b9BPgZzjw566y1VbHlPwDuwgn96oGpYQcuIhC7EMaL1tpjsfu7gGKcuWIeM8acgzM52vcS9+uJjEwtckkXAZxZJ8+21p6NE+r/xPFzxftj94e+7n04jZpunDlJADDGFMa6TxjyPP04U9y+ApyJ043zIZyr4YgknYJc0sWzwEdjs04GgV/iXJDExC6JBnArzkx3rwOrjDGzYstvw5nkzAKlxpgzY8s/jzOP97CMMV/H6XP/Cc4shMsm9lcSGRsFuaQFa+3jwKM4Ib0Z2IBzdZ9jwJeMMVtwrg355dh1LW8D1sSWXwzcEZsF8mbgQWPM2zit7a+e5Md+F7jBGLMBZ8bOjyXidxMZjc5akbQVa3E/b62d5XIpIgmlFrmIiMepRS4i4nFqkYuIeJyCXETE4xTkIiIepyAXEfE4BbmIiMf9f54JLFQ4EKNQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "losses, params = train(init_params, update_weights, x, y_true, batch_size=128, num_epoch=20, learning_rate=0.1, plot_loss=True)\n",
    "W_learned, b_learned = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "losses": [
        1.1552293486084748,
        0.8198945712895466,
        0.6485110681444918,
        0.5452068560808377,
        0.47591045919700137,
        0.42593945021049606,
        0.38803473839081776,
        0.35820058895380935,
        0.3340507639470303,
        0.3140680235661119,
        0.29723884690688557,
        0.2828587419538791,
        0.2704217701953249,
        0.2595545436187525,
        0.24997505789612773,
        0.24146606394217507,
        0.2338572977018303,
        0.22701329662697362,
        0.2208248467699275,
        0.21520285215925497
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('losses', losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(x, W, b):\n",
    "    \"\"\"\n",
    "        Make predictions for the given inputs x.\n",
    "\n",
    "        x: np.array [N, d], inputs\n",
    "        W: np.array, shape=[d, k], weights\n",
    "        b: np.array, shape=[1, k], biases\n",
    "\n",
    "        return: np.array of labels with shape [N]\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return np.argmax(softmax(linear(x, W, b)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = make_prediction(x, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "y_pred.shape": [
        2247
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('y_pred.shape', y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9844236760124611\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(dataset.target, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "accuracy": 0.9844236760124611
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 4 Let's add more layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Similarly to the single-layer neural network, the two-layer neural network makes the following transformations:\n",
    "$$ \\mathbf{z}^{(1)} = W^{(1)}\\mathbf{x} + \\mathbf{b^{(1)}} $$\n",
    "$$ \\mathbf{a}^{(1)} = f\\left(\\mathbf{z}^{(1)}\\right)$$\n",
    "\n",
    "$$ \\mathbf{z}^{(2)} = W^{(2)}\\mathbf{a^{(1)}} + \\mathbf{b^{(2)}} $$\n",
    "$$ \\mathbf{\\hat{y}} = \\text{softmax}\\left(\\mathbf{z}^{(2)}\\right)$$\n",
    "where $f$ is the activation of the first linear layer. In our network we will use the Rectified Linear Unit (ReLU) activation function defined as:\n",
    "\n",
    "$$ f(x)=\\text{ReLU(x)} = \\max(0,x) $$\n",
    "\n",
    "The corresponding formulas for computing the loss gradients are then as follows:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial  \\mathbf{z}^{(2)}}{\\partial W^{(2)}}$$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(2)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{b}^{(2)}}$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial \\mathbf{z}^{(1)}} \\cdot \\frac{\\partial\\mathbf{z}^{(1)}}{\\partial W^{(1)}}  $$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial \\mathbf{z}^{(1)}} \\cdot \\frac{\\partial\\mathbf{z}^{(1)}}{\\partial \\mathbf{b}^{(1)}}  $$\n",
    "\n",
    "In these notations $W^{(1)} \\in \\mathbb{R}^{m\\times d_2}$, $W^{(2)} \\in \\mathbb{R}^{d_2\\times k}$,  $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{d_2}$ and $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{k}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 4.1 \n",
    "#### (2 points)\n",
    "\n",
    "Simlarly to sections 3.2 and 3.6, perform the forward and backward propagation pass through the ReLU function (using its' definition).\n",
    "\n",
    "---\n",
    "\n",
    "__Notes__:\n",
    "- In `relu_backward` the variable `a_grad` stands for  $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a^{(1)}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "        Compute relu activation as max(z, 0)\n",
    "        \n",
    "        z: np.array, shape=[m, d]\n",
    "\n",
    "        Return: np.array, shape=[m, d]\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return np.maximum(np.zeros_like(z), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(z, a_grad):\n",
    "    \"\"\"\n",
    "        Return gradient of loss with respect to input z given the output gradient.\n",
    "\n",
    "        z: np.array, shape=[m, d] input to relu, not the output\n",
    "        \n",
    "        return: gradient dL/dz np.array, shape=[m, d]\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return np.array([np.array(list(map(lambda z, grad: 0 if z < 0 else grad, z_, a_grad_))) for z_, a_grad_ in zip(z, a_grad)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 4.2 \n",
    "#### (1 point)\n",
    "\n",
    "Perform the forward propagation of the two-layer neural network by completing the code in `neural_network`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def neural_network(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "        Use layers created above to define two-layer neural network with relu activation.\n",
    "\n",
    "        x: np.array, shape=[m, n_features_tst]\n",
    "        W1: np.array, shape=[m, d2] first layer weights\n",
    "        W2: np.array, shape=[d2, k] second layer weights\n",
    "        b1: np.array, shape=[1, d2] first layer bias\n",
    "        b2: np.array, shape=[1, k] second layer bias\n",
    "\n",
    "        return: tuple, output of the linear layer, output of the relu, output of the final linear layer\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return linear(x, W1, b1), relu(linear(x, W1, b1)), linear(relu(linear(x, W1, b1)), W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 4.3\n",
    "#### (3 points)\n",
    "\n",
    "Compute the code in `neural_network_backward` that computes gradients of the loss function with respect to every weight.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- You should call the already implemented backward functions in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def neural_network_backward(x, W1, b1, W2, b2, z1, a1, z2, z2_grad):\n",
    "    \"\"\"\n",
    "        Compute gradients of loss with respect to inputs x, W1, b1, W2, and b2.\n",
    "\n",
    "        x: np.array, shape=[m, d] input\n",
    "        W1: np.array, shape=[m, d2] first layer weights\n",
    "        b1: np.array, shape=[1, d2] first layer bias\n",
    "        W2: np.array, shape=[d2, k] second layer weights\n",
    "        b2: np.array, shape=[1, k] second layer bias\n",
    "\n",
    "        z1, z2, z3: np.array activations of linear, relu, and linear layers\n",
    "        z3_grad: np.array, shape=[m, k] gradient of loss with respect to network output dL/dz3\n",
    "\n",
    "        return: tuple: gradients dL/dW1, dL/db1, dL,dW2, dL/db2\n",
    "    \"\"\"\n",
    "    a1_grad, W2_grad, b2_grad = linear_backward(a1, W2, z2_grad)\n",
    "    z1_grad = relu_backward(z1, a1_grad)\n",
    "    x_grad, W1_grad, b1_grad = linear_backward(x, W1, z1_grad)\n",
    "\n",
    "    return W1_grad, b1_grad, W2_grad, b2_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 4.4\n",
    "#### (3 points)\n",
    "\n",
    "Complete the code and `update_neural_network_weights`.\n",
    "\n",
    "---\n",
    "__Notes__:\n",
    "- The weights initialization is provided for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_neural_network_params(x, y_true):\n",
    "    \"\"\"\n",
    "        Initialize weights and biases of two layer neural network, given input data.\n",
    "        \n",
    "        x: np.array, shape=[m, d] input\n",
    "        y_true: np.array, shape=[m, k] true label one-hot\n",
    "        \n",
    "        return: list of np.array parameters\n",
    "    \"\"\"\n",
    "    n_features = x.shape[1]\n",
    "    n_classes = y_true.shape[1]\n",
    "    n_middle = n_classes * 4  # we arbitrarily pick the middle layer size to be 4 times bigger than the output\n",
    "\n",
    "    # Initialize W from normal distribution and b with zeros\n",
    "    W1 = np.random.randn(n_features, n_middle) / n_features\n",
    "    W2 = np.random.randn(n_middle, n_classes) / n_middle\n",
    "    b1 = np.zeros((1, n_middle))\n",
    "    b2 = np.zeros((1, n_classes))\n",
    "    \n",
    "    return [W1, W2, b1, b2]\n",
    "\n",
    "def update_neural_network_weights(params, x, y_true, alpha, lam=0.001):\n",
    "    \"\"\"\n",
    "        Given neural network parameters, update parameters and return loss.\n",
    "        \n",
    "        params: list of np.array parameters\n",
    "        x: np.array, shape=[m, d] input\n",
    "        y_true: np.array, shape=[m, k] output labels one-hot\n",
    "        alpha: learning rate (float)\n",
    "        lam: lambda regularization constant (float)\n",
    "        \n",
    "        return: float sum of cross entropy loss and L2 regularization losses\n",
    "    \"\"\"\n",
    "    # place your code below\n",
    "    W1, W2, b1, b2 = params\n",
    "    z1, a1, z2 = neural_network(x, W1, b1, W2, b2)\n",
    "    y = softmax(z2)\n",
    "    loss = cross_entropy(y, y_true)\n",
    "    z2_grad = softmax_cross_entropy_backward(z2, y_true)\n",
    "    W1_grad, b1_grad, W2_grad, b2_grad = neural_network_backward(x, W1, b1, W2, b2, z1, a1, z2, z2_grad)\n",
    "    reg_loss, [W1_reg_grad, W2_reg_grad, b1_reg_grad, b2_reg_grad] = regularization(params, lam)\n",
    "    # place your code above\n",
    "    W1 -= alpha * (W1_grad + W1_reg_grad)\n",
    "    W2 -= alpha * (W2_grad + W2_reg_grad)\n",
    "    b1 -= alpha * (b1_grad + b1_reg_grad)\n",
    "    b2 -= alpha * (b2_grad + b2_reg_grad)\n",
    "    return loss + reg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 4.5\n",
    "#### (1 point)\n",
    "Complete the code in `make_prediction_network` function similarly to the section 3.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_prediction_network(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "        Make predictions for the given inputs x.\n",
    "\n",
    "        x: np.array, shape=[m, d] input\n",
    "        W1: np.array, shape=[m, d2] first layer weights\n",
    "        b1: np.array, shape=[1, d2] first layer bias\n",
    "        W2: np.array, shape=[d2, k] second layer weights\n",
    "        b2: np.array, shape=[1, k] second layer bias\n",
    "\n",
    "        return: np.array of labels with shape [N]\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    return np.argmax(neural_network(x, W1, b1, W2, b2)[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Loss: 1.136\n",
      "Epoch: 1     Loss: 1.109\n",
      "Epoch: 2     Loss: 1.086\n",
      "Epoch: 3     Loss: 1.065\n",
      "Epoch: 4     Loss: 1.044\n",
      "Epoch: 5     Loss: 1.024\n",
      "Epoch: 6     Loss: 1.001\n",
      "Epoch: 7     Loss: 0.977\n",
      "Epoch: 8     Loss: 0.950\n",
      "Epoch: 9     Loss: 0.921\n",
      "Epoch: 10    Loss: 0.892\n",
      "Epoch: 11    Loss: 0.859\n",
      "Epoch: 12    Loss: 0.824\n",
      "Epoch: 13    Loss: 0.787\n",
      "Epoch: 14    Loss: 0.750\n",
      "Epoch: 15    Loss: 0.710\n",
      "Epoch: 16    Loss: 0.675\n",
      "Epoch: 17    Loss: 0.637\n",
      "Epoch: 18    Loss: 0.601\n",
      "Epoch: 19    Loss: 0.564\n",
      "Epoch: 20    Loss: 0.530\n",
      "Epoch: 21    Loss: 0.495\n",
      "Epoch: 22    Loss: 0.466\n",
      "Epoch: 23    Loss: 0.436\n",
      "Epoch: 24    Loss: 0.404\n",
      "Epoch: 25    Loss: 0.376\n",
      "Epoch: 26    Loss: 0.350\n",
      "Epoch: 27    Loss: 0.322\n",
      "Epoch: 28    Loss: 0.301\n",
      "Epoch: 29    Loss: 0.276\n",
      "Epoch: 30    Loss: 0.254\n",
      "Epoch: 31    Loss: 0.235\n",
      "Epoch: 32    Loss: 0.214\n",
      "Epoch: 33    Loss: 0.200\n",
      "Epoch: 34    Loss: 0.182\n",
      "Epoch: 35    Loss: 0.171\n",
      "Epoch: 36    Loss: 0.157\n",
      "Epoch: 37    Loss: 0.148\n",
      "Epoch: 38    Loss: 0.137\n",
      "Epoch: 39    Loss: 0.128\n",
      "Epoch: 40    Loss: 0.120\n",
      "Epoch: 41    Loss: 0.114\n",
      "Epoch: 42    Loss: 0.108\n",
      "Epoch: 43    Loss: 0.102\n",
      "Epoch: 44    Loss: 0.097\n",
      "Epoch: 45    Loss: 0.092\n",
      "Epoch: 46    Loss: 0.089\n",
      "Epoch: 47    Loss: 0.085\n",
      "Epoch: 48    Loss: 0.082\n",
      "Epoch: 49    Loss: 0.078\n",
      "Epoch: 50    Loss: 0.076\n",
      "Epoch: 51    Loss: 0.073\n",
      "Epoch: 52    Loss: 0.071\n",
      "Epoch: 53    Loss: 0.069\n",
      "Epoch: 54    Loss: 0.067\n",
      "Epoch: 55    Loss: 0.066\n",
      "Epoch: 56    Loss: 0.064\n",
      "Epoch: 57    Loss: 0.062\n",
      "Epoch: 58    Loss: 0.061\n",
      "Epoch: 59    Loss: 0.060\n",
      "Epoch: 60    Loss: 0.058\n",
      "Epoch: 61    Loss: 0.057\n",
      "Epoch: 62    Loss: 0.056\n",
      "Epoch: 63    Loss: 0.055\n",
      "Epoch: 64    Loss: 0.054\n",
      "Epoch: 65    Loss: 0.053\n",
      "Epoch: 66    Loss: 0.053\n",
      "Epoch: 67    Loss: 0.052\n",
      "Epoch: 68    Loss: 0.051\n",
      "Epoch: 69    Loss: 0.051\n",
      "Epoch: 70    Loss: 0.050\n",
      "Epoch: 71    Loss: 0.049\n",
      "Epoch: 72    Loss: 0.049\n",
      "Epoch: 73    Loss: 0.048\n",
      "Epoch: 74    Loss: 0.048\n",
      "Epoch: 75    Loss: 0.047\n",
      "Epoch: 76    Loss: 0.047\n",
      "Epoch: 77    Loss: 0.046\n",
      "Epoch: 78    Loss: 0.046\n",
      "Epoch: 79    Loss: 0.045\n",
      "Epoch: 80    Loss: 0.045\n",
      "Epoch: 81    Loss: 0.045\n",
      "Epoch: 82    Loss: 0.044\n",
      "Epoch: 83    Loss: 0.044\n",
      "Epoch: 84    Loss: 0.044\n",
      "Epoch: 85    Loss: 0.043\n",
      "Epoch: 86    Loss: 0.043\n",
      "Epoch: 87    Loss: 0.043\n",
      "Epoch: 88    Loss: 0.043\n",
      "Epoch: 89    Loss: 0.042\n",
      "Epoch: 90    Loss: 0.042\n",
      "Epoch: 91    Loss: 0.042\n",
      "Epoch: 92    Loss: 0.042\n",
      "Epoch: 93    Loss: 0.041\n",
      "Epoch: 94    Loss: 0.041\n",
      "Epoch: 95    Loss: 0.041\n",
      "Epoch: 96    Loss: 0.041\n",
      "Epoch: 97    Loss: 0.041\n",
      "Epoch: 98    Loss: 0.041\n",
      "Epoch: 99    Loss: 0.040\n",
      "Epoch: 100   Loss: 0.040\n",
      "Epoch: 101   Loss: 0.040\n",
      "Epoch: 102   Loss: 0.040\n",
      "Epoch: 103   Loss: 0.040\n",
      "Epoch: 104   Loss: 0.040\n",
      "Epoch: 105   Loss: 0.039\n",
      "Epoch: 106   Loss: 0.039\n",
      "Epoch: 107   Loss: 0.039\n",
      "Epoch: 108   Loss: 0.039\n",
      "Epoch: 109   Loss: 0.039\n",
      "Epoch: 110   Loss: 0.039\n",
      "Epoch: 111   Loss: 0.039\n",
      "Epoch: 112   Loss: 0.039\n",
      "Epoch: 113   Loss: 0.039\n",
      "Epoch: 114   Loss: 0.038\n",
      "Epoch: 115   Loss: 0.038\n",
      "Epoch: 116   Loss: 0.038\n",
      "Epoch: 117   Loss: 0.038\n",
      "Epoch: 118   Loss: 0.038\n",
      "Epoch: 119   Loss: 0.038\n",
      "Epoch: 120   Loss: 0.038\n",
      "Epoch: 121   Loss: 0.038\n",
      "Epoch: 122   Loss: 0.038\n",
      "Epoch: 123   Loss: 0.038\n",
      "Epoch: 124   Loss: 0.038\n",
      "Epoch: 125   Loss: 0.038\n",
      "Epoch: 126   Loss: 0.037\n",
      "Epoch: 127   Loss: 0.037\n",
      "Epoch: 128   Loss: 0.037\n",
      "Epoch: 129   Loss: 0.037\n",
      "Epoch: 130   Loss: 0.037\n",
      "Epoch: 131   Loss: 0.037\n",
      "Epoch: 132   Loss: 0.037\n",
      "Epoch: 133   Loss: 0.037\n",
      "Epoch: 134   Loss: 0.037\n",
      "Epoch: 135   Loss: 0.037\n",
      "Epoch: 136   Loss: 0.037\n",
      "Epoch: 137   Loss: 0.037\n",
      "Epoch: 138   Loss: 0.037\n",
      "Epoch: 139   Loss: 0.037\n",
      "Epoch: 140   Loss: 0.037\n",
      "Epoch: 141   Loss: 0.037\n",
      "Epoch: 142   Loss: 0.037\n",
      "Epoch: 143   Loss: 0.037\n",
      "Epoch: 144   Loss: 0.037\n",
      "Epoch: 145   Loss: 0.036\n",
      "Epoch: 146   Loss: 0.036\n",
      "Epoch: 147   Loss: 0.036\n",
      "Epoch: 148   Loss: 0.036\n",
      "Epoch: 149   Loss: 0.036\n",
      "Epoch: 150   Loss: 0.036\n",
      "Epoch: 151   Loss: 0.036\n",
      "Epoch: 152   Loss: 0.036\n",
      "Epoch: 153   Loss: 0.036\n",
      "Epoch: 154   Loss: 0.036\n",
      "Epoch: 155   Loss: 0.036\n",
      "Epoch: 156   Loss: 0.036\n",
      "Epoch: 157   Loss: 0.036\n",
      "Epoch: 158   Loss: 0.036\n",
      "Epoch: 159   Loss: 0.036\n",
      "Epoch: 160   Loss: 0.036\n",
      "Epoch: 161   Loss: 0.036\n",
      "Epoch: 162   Loss: 0.036\n",
      "Epoch: 163   Loss: 0.036\n",
      "Epoch: 164   Loss: 0.036\n",
      "Epoch: 165   Loss: 0.036\n",
      "Epoch: 166   Loss: 0.036\n",
      "Epoch: 167   Loss: 0.036\n",
      "Epoch: 168   Loss: 0.036\n",
      "Epoch: 169   Loss: 0.036\n",
      "Epoch: 170   Loss: 0.036\n",
      "Epoch: 171   Loss: 0.036\n",
      "Epoch: 172   Loss: 0.036\n",
      "Epoch: 173   Loss: 0.036\n",
      "Epoch: 174   Loss: 0.036\n",
      "Epoch: 175   Loss: 0.036\n",
      "Epoch: 176   Loss: 0.036\n",
      "Epoch: 177   Loss: 0.036\n",
      "Epoch: 178   Loss: 0.036\n",
      "Epoch: 179   Loss: 0.036\n",
      "Epoch: 180   Loss: 0.036\n",
      "Epoch: 181   Loss: 0.035\n",
      "Epoch: 182   Loss: 0.035\n",
      "Epoch: 183   Loss: 0.035\n",
      "Epoch: 184   Loss: 0.035\n",
      "Epoch: 185   Loss: 0.035\n",
      "Epoch: 186   Loss: 0.035\n",
      "Epoch: 187   Loss: 0.035\n",
      "Epoch: 188   Loss: 0.035\n",
      "Epoch: 189   Loss: 0.035\n",
      "Epoch: 190   Loss: 0.035\n",
      "Epoch: 191   Loss: 0.035\n",
      "Epoch: 192   Loss: 0.035\n",
      "Epoch: 193   Loss: 0.035\n",
      "Epoch: 194   Loss: 0.035\n",
      "Epoch: 195   Loss: 0.035\n",
      "Epoch: 196   Loss: 0.035\n",
      "Epoch: 197   Loss: 0.035\n",
      "Epoch: 198   Loss: 0.035\n",
      "Epoch: 199   Loss: 0.035\n",
      "Epoch: 200   Loss: 0.035\n",
      "Epoch: 201   Loss: 0.035\n",
      "Epoch: 202   Loss: 0.035\n",
      "Epoch: 203   Loss: 0.035\n",
      "Epoch: 204   Loss: 0.035\n",
      "Epoch: 205   Loss: 0.035\n",
      "Epoch: 206   Loss: 0.035\n",
      "Epoch: 207   Loss: 0.035\n",
      "Epoch: 208   Loss: 0.035\n",
      "Epoch: 209   Loss: 0.035\n",
      "Epoch: 210   Loss: 0.035\n",
      "Epoch: 211   Loss: 0.035\n",
      "Epoch: 212   Loss: 0.035\n",
      "Epoch: 213   Loss: 0.035\n",
      "Epoch: 214   Loss: 0.035\n",
      "Epoch: 215   Loss: 0.035\n",
      "Epoch: 216   Loss: 0.035\n",
      "Epoch: 217   Loss: 0.035\n",
      "Epoch: 218   Loss: 0.035\n",
      "Epoch: 219   Loss: 0.035\n",
      "Epoch: 220   Loss: 0.035\n",
      "Epoch: 221   Loss: 0.035\n",
      "Epoch: 222   Loss: 0.035\n",
      "Epoch: 223   Loss: 0.035\n",
      "Epoch: 224   Loss: 0.035\n",
      "Epoch: 225   Loss: 0.035\n",
      "Epoch: 226   Loss: 0.035\n",
      "Epoch: 227   Loss: 0.035\n",
      "Epoch: 228   Loss: 0.035\n",
      "Epoch: 229   Loss: 0.035\n",
      "Epoch: 230   Loss: 0.035\n",
      "Epoch: 231   Loss: 0.035\n",
      "Epoch: 232   Loss: 0.035\n",
      "Epoch: 233   Loss: 0.035\n",
      "Epoch: 234   Loss: 0.035\n",
      "Epoch: 235   Loss: 0.035\n",
      "Epoch: 236   Loss: 0.035\n",
      "Epoch: 237   Loss: 0.035\n",
      "Epoch: 238   Loss: 0.035\n",
      "Epoch: 239   Loss: 0.035\n",
      "Epoch: 240   Loss: 0.035\n",
      "Epoch: 241   Loss: 0.035\n",
      "Epoch: 242   Loss: 0.035\n",
      "Epoch: 243   Loss: 0.035\n",
      "Epoch: 244   Loss: 0.035\n",
      "Epoch: 245   Loss: 0.035\n",
      "Epoch: 246   Loss: 0.035\n",
      "Epoch: 247   Loss: 0.035\n",
      "Epoch: 248   Loss: 0.035\n",
      "Epoch: 249   Loss: 0.035\n",
      "Epoch: 250   Loss: 0.035\n",
      "Epoch: 251   Loss: 0.035\n",
      "Epoch: 252   Loss: 0.035\n",
      "Epoch: 253   Loss: 0.035\n",
      "Epoch: 254   Loss: 0.035\n",
      "Epoch: 255   Loss: 0.035\n",
      "Epoch: 256   Loss: 0.035\n",
      "Epoch: 257   Loss: 0.035\n",
      "Epoch: 258   Loss: 0.035\n",
      "Epoch: 259   Loss: 0.035\n",
      "Epoch: 260   Loss: 0.035\n",
      "Epoch: 261   Loss: 0.035\n",
      "Epoch: 262   Loss: 0.035\n",
      "Epoch: 263   Loss: 0.035\n",
      "Epoch: 264   Loss: 0.035\n",
      "Epoch: 265   Loss: 0.035\n",
      "Epoch: 266   Loss: 0.035\n",
      "Epoch: 267   Loss: 0.035\n",
      "Epoch: 268   Loss: 0.035\n",
      "Epoch: 269   Loss: 0.035\n",
      "Epoch: 270   Loss: 0.035\n",
      "Epoch: 271   Loss: 0.035\n",
      "Epoch: 272   Loss: 0.035\n",
      "Epoch: 273   Loss: 0.035\n",
      "Epoch: 274   Loss: 0.035\n",
      "Epoch: 275   Loss: 0.035\n",
      "Epoch: 276   Loss: 0.035\n",
      "Epoch: 277   Loss: 0.035\n",
      "Epoch: 278   Loss: 0.035\n",
      "Epoch: 279   Loss: 0.035\n",
      "Epoch: 280   Loss: 0.035\n",
      "Epoch: 281   Loss: 0.035\n",
      "Epoch: 282   Loss: 0.035\n",
      "Epoch: 283   Loss: 0.035\n",
      "Epoch: 284   Loss: 0.035\n",
      "Epoch: 285   Loss: 0.035\n",
      "Epoch: 286   Loss: 0.035\n",
      "Epoch: 287   Loss: 0.035\n",
      "Epoch: 288   Loss: 0.035\n",
      "Epoch: 289   Loss: 0.035\n",
      "Epoch: 290   Loss: 0.035\n",
      "Epoch: 291   Loss: 0.035\n",
      "Epoch: 292   Loss: 0.035\n",
      "Epoch: 293   Loss: 0.034\n",
      "Epoch: 294   Loss: 0.034\n",
      "Epoch: 295   Loss: 0.034\n",
      "Epoch: 296   Loss: 0.034\n",
      "Epoch: 297   Loss: 0.034\n",
      "Epoch: 298   Loss: 0.034\n",
      "Epoch: 299   Loss: 0.034\n",
      "Epoch: 300   Loss: 0.034\n",
      "Epoch: 301   Loss: 0.034\n",
      "Epoch: 302   Loss: 0.034\n",
      "Epoch: 303   Loss: 0.034\n",
      "Epoch: 304   Loss: 0.034\n",
      "Epoch: 305   Loss: 0.034\n",
      "Epoch: 306   Loss: 0.034\n",
      "Epoch: 307   Loss: 0.034\n",
      "Epoch: 308   Loss: 0.034\n",
      "Epoch: 309   Loss: 0.034\n",
      "Epoch: 310   Loss: 0.034\n",
      "Epoch: 311   Loss: 0.034\n",
      "Epoch: 312   Loss: 0.034\n",
      "Epoch: 313   Loss: 0.034\n",
      "Epoch: 314   Loss: 0.034\n",
      "Epoch: 315   Loss: 0.034\n",
      "Epoch: 316   Loss: 0.034\n",
      "Epoch: 317   Loss: 0.034\n",
      "Epoch: 318   Loss: 0.034\n",
      "Epoch: 319   Loss: 0.034\n",
      "Epoch: 320   Loss: 0.034\n",
      "Epoch: 321   Loss: 0.034\n",
      "Epoch: 322   Loss: 0.034\n",
      "Epoch: 323   Loss: 0.034\n",
      "Epoch: 324   Loss: 0.034\n",
      "Epoch: 325   Loss: 0.034\n",
      "Epoch: 326   Loss: 0.034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 327   Loss: 0.034\n",
      "Epoch: 328   Loss: 0.034\n",
      "Epoch: 329   Loss: 0.034\n",
      "Epoch: 330   Loss: 0.034\n",
      "Epoch: 331   Loss: 0.034\n",
      "Epoch: 332   Loss: 0.034\n",
      "Epoch: 333   Loss: 0.034\n",
      "Epoch: 334   Loss: 0.034\n",
      "Epoch: 335   Loss: 0.034\n",
      "Epoch: 336   Loss: 0.034\n",
      "Epoch: 337   Loss: 0.034\n",
      "Epoch: 338   Loss: 0.034\n",
      "Epoch: 339   Loss: 0.034\n",
      "Epoch: 340   Loss: 0.034\n",
      "Epoch: 341   Loss: 0.034\n",
      "Epoch: 342   Loss: 0.034\n",
      "Epoch: 343   Loss: 0.034\n",
      "Epoch: 344   Loss: 0.034\n",
      "Epoch: 345   Loss: 0.034\n",
      "Epoch: 346   Loss: 0.034\n",
      "Epoch: 347   Loss: 0.034\n",
      "Epoch: 348   Loss: 0.034\n",
      "Epoch: 349   Loss: 0.034\n",
      "Epoch: 350   Loss: 0.034\n",
      "Epoch: 351   Loss: 0.034\n",
      "Epoch: 352   Loss: 0.034\n",
      "Epoch: 353   Loss: 0.034\n",
      "Epoch: 354   Loss: 0.034\n",
      "Epoch: 355   Loss: 0.034\n",
      "Epoch: 356   Loss: 0.034\n",
      "Epoch: 357   Loss: 0.034\n",
      "Epoch: 358   Loss: 0.034\n",
      "Epoch: 359   Loss: 0.034\n",
      "Epoch: 360   Loss: 0.034\n",
      "Epoch: 361   Loss: 0.034\n",
      "Epoch: 362   Loss: 0.034\n",
      "Epoch: 363   Loss: 0.034\n",
      "Epoch: 364   Loss: 0.034\n",
      "Epoch: 365   Loss: 0.034\n",
      "Epoch: 366   Loss: 0.034\n",
      "Epoch: 367   Loss: 0.034\n",
      "Epoch: 368   Loss: 0.034\n",
      "Epoch: 369   Loss: 0.034\n",
      "Epoch: 370   Loss: 0.034\n",
      "Epoch: 371   Loss: 0.034\n",
      "Epoch: 372   Loss: 0.034\n",
      "Epoch: 373   Loss: 0.034\n",
      "Epoch: 374   Loss: 0.034\n",
      "Epoch: 375   Loss: 0.034\n",
      "Epoch: 376   Loss: 0.034\n",
      "Epoch: 377   Loss: 0.034\n",
      "Epoch: 378   Loss: 0.034\n",
      "Epoch: 379   Loss: 0.034\n",
      "Epoch: 380   Loss: 0.034\n",
      "Epoch: 381   Loss: 0.034\n",
      "Epoch: 382   Loss: 0.034\n",
      "Epoch: 383   Loss: 0.034\n",
      "Epoch: 384   Loss: 0.034\n",
      "Epoch: 385   Loss: 0.034\n",
      "Epoch: 386   Loss: 0.034\n",
      "Epoch: 387   Loss: 0.034\n",
      "Epoch: 388   Loss: 0.034\n",
      "Epoch: 389   Loss: 0.034\n",
      "Epoch: 390   Loss: 0.034\n",
      "Epoch: 391   Loss: 0.034\n",
      "Epoch: 392   Loss: 0.034\n",
      "Epoch: 393   Loss: 0.034\n",
      "Epoch: 394   Loss: 0.034\n",
      "Epoch: 395   Loss: 0.034\n",
      "Epoch: 396   Loss: 0.034\n",
      "Epoch: 397   Loss: 0.034\n",
      "Epoch: 398   Loss: 0.034\n",
      "Epoch: 399   Loss: 0.034\n",
      "Epoch: 400   Loss: 0.034\n",
      "Epoch: 401   Loss: 0.034\n",
      "Epoch: 402   Loss: 0.034\n",
      "Epoch: 403   Loss: 0.034\n",
      "Epoch: 404   Loss: 0.034\n",
      "Epoch: 405   Loss: 0.034\n",
      "Epoch: 406   Loss: 0.034\n",
      "Epoch: 407   Loss: 0.034\n",
      "Epoch: 408   Loss: 0.034\n",
      "Epoch: 409   Loss: 0.034\n",
      "Epoch: 410   Loss: 0.034\n",
      "Epoch: 411   Loss: 0.034\n",
      "Epoch: 412   Loss: 0.034\n",
      "Epoch: 413   Loss: 0.034\n",
      "Epoch: 414   Loss: 0.034\n",
      "Epoch: 415   Loss: 0.034\n",
      "Epoch: 416   Loss: 0.034\n",
      "Epoch: 417   Loss: 0.034\n",
      "Epoch: 418   Loss: 0.034\n",
      "Epoch: 419   Loss: 0.034\n",
      "Epoch: 420   Loss: 0.034\n",
      "Epoch: 421   Loss: 0.034\n",
      "Epoch: 422   Loss: 0.034\n",
      "Epoch: 423   Loss: 0.034\n",
      "Epoch: 424   Loss: 0.034\n",
      "Epoch: 425   Loss: 0.034\n",
      "Epoch: 426   Loss: 0.034\n",
      "Epoch: 427   Loss: 0.034\n",
      "Epoch: 428   Loss: 0.034\n",
      "Epoch: 429   Loss: 0.034\n",
      "Epoch: 430   Loss: 0.034\n",
      "Epoch: 431   Loss: 0.034\n",
      "Epoch: 432   Loss: 0.034\n",
      "Epoch: 433   Loss: 0.034\n",
      "Epoch: 434   Loss: 0.034\n",
      "Epoch: 435   Loss: 0.034\n",
      "Epoch: 436   Loss: 0.034\n",
      "Epoch: 437   Loss: 0.034\n",
      "Epoch: 438   Loss: 0.034\n",
      "Epoch: 439   Loss: 0.034\n",
      "Epoch: 440   Loss: 0.034\n",
      "Epoch: 441   Loss: 0.034\n",
      "Epoch: 442   Loss: 0.034\n",
      "Epoch: 443   Loss: 0.034\n",
      "Epoch: 444   Loss: 0.034\n",
      "Epoch: 445   Loss: 0.034\n",
      "Epoch: 446   Loss: 0.034\n",
      "Epoch: 447   Loss: 0.034\n",
      "Epoch: 448   Loss: 0.034\n",
      "Epoch: 449   Loss: 0.034\n",
      "Epoch: 450   Loss: 0.034\n",
      "Epoch: 451   Loss: 0.034\n",
      "Epoch: 452   Loss: 0.034\n",
      "Epoch: 453   Loss: 0.034\n",
      "Epoch: 454   Loss: 0.034\n",
      "Epoch: 455   Loss: 0.034\n",
      "Epoch: 456   Loss: 0.034\n",
      "Epoch: 457   Loss: 0.034\n",
      "Epoch: 458   Loss: 0.034\n",
      "Epoch: 459   Loss: 0.034\n",
      "Epoch: 460   Loss: 0.034\n",
      "Epoch: 461   Loss: 0.034\n",
      "Epoch: 462   Loss: 0.034\n",
      "Epoch: 463   Loss: 0.034\n",
      "Epoch: 464   Loss: 0.034\n",
      "Epoch: 465   Loss: 0.034\n",
      "Epoch: 466   Loss: 0.034\n",
      "Epoch: 467   Loss: 0.034\n",
      "Epoch: 468   Loss: 0.034\n",
      "Epoch: 469   Loss: 0.034\n",
      "Epoch: 470   Loss: 0.034\n",
      "Epoch: 471   Loss: 0.034\n",
      "Epoch: 472   Loss: 0.034\n",
      "Epoch: 473   Loss: 0.034\n",
      "Epoch: 474   Loss: 0.034\n",
      "Epoch: 475   Loss: 0.034\n",
      "Epoch: 476   Loss: 0.034\n",
      "Epoch: 477   Loss: 0.034\n",
      "Epoch: 478   Loss: 0.034\n",
      "Epoch: 479   Loss: 0.034\n",
      "Epoch: 480   Loss: 0.034\n",
      "Epoch: 481   Loss: 0.034\n",
      "Epoch: 482   Loss: 0.034\n",
      "Epoch: 483   Loss: 0.034\n",
      "Epoch: 484   Loss: 0.034\n",
      "Epoch: 485   Loss: 0.034\n",
      "Epoch: 486   Loss: 0.034\n",
      "Epoch: 487   Loss: 0.034\n",
      "Epoch: 488   Loss: 0.034\n",
      "Epoch: 489   Loss: 0.034\n",
      "Epoch: 490   Loss: 0.034\n",
      "Epoch: 491   Loss: 0.034\n",
      "Epoch: 492   Loss: 0.034\n",
      "Epoch: 493   Loss: 0.034\n",
      "Epoch: 494   Loss: 0.034\n",
      "Epoch: 495   Loss: 0.034\n",
      "Epoch: 496   Loss: 0.034\n",
      "Epoch: 497   Loss: 0.034\n",
      "Epoch: 498   Loss: 0.034\n",
      "Epoch: 499   Loss: 0.034\n",
      "Epoch: 500   Loss: 0.034\n",
      "Epoch: 501   Loss: 0.034\n",
      "Epoch: 502   Loss: 0.034\n",
      "Epoch: 503   Loss: 0.034\n",
      "Epoch: 504   Loss: 0.034\n",
      "Epoch: 505   Loss: 0.034\n",
      "Epoch: 506   Loss: 0.034\n",
      "Epoch: 507   Loss: 0.034\n",
      "Epoch: 508   Loss: 0.034\n",
      "Epoch: 509   Loss: 0.034\n",
      "Epoch: 510   Loss: 0.034\n",
      "Epoch: 511   Loss: 0.034\n",
      "Epoch: 512   Loss: 0.034\n",
      "Epoch: 513   Loss: 0.034\n",
      "Epoch: 514   Loss: 0.034\n",
      "Epoch: 515   Loss: 0.034\n",
      "Epoch: 516   Loss: 0.034\n",
      "Epoch: 517   Loss: 0.034\n",
      "Epoch: 518   Loss: 0.034\n",
      "Epoch: 519   Loss: 0.034\n",
      "Epoch: 520   Loss: 0.034\n",
      "Epoch: 521   Loss: 0.034\n",
      "Epoch: 522   Loss: 0.034\n",
      "Epoch: 523   Loss: 0.034\n",
      "Epoch: 524   Loss: 0.034\n",
      "Epoch: 525   Loss: 0.034\n",
      "Epoch: 526   Loss: 0.034\n",
      "Epoch: 527   Loss: 0.034\n",
      "Epoch: 528   Loss: 0.034\n",
      "Epoch: 529   Loss: 0.034\n",
      "Epoch: 530   Loss: 0.034\n",
      "Epoch: 531   Loss: 0.034\n",
      "Epoch: 532   Loss: 0.034\n",
      "Epoch: 533   Loss: 0.034\n",
      "Epoch: 534   Loss: 0.034\n",
      "Epoch: 535   Loss: 0.034\n",
      "Epoch: 536   Loss: 0.034\n",
      "Epoch: 537   Loss: 0.034\n",
      "Epoch: 538   Loss: 0.034\n",
      "Epoch: 539   Loss: 0.034\n",
      "Epoch: 540   Loss: 0.034\n",
      "Epoch: 541   Loss: 0.034\n",
      "Epoch: 542   Loss: 0.034\n",
      "Epoch: 543   Loss: 0.034\n",
      "Epoch: 544   Loss: 0.034\n",
      "Epoch: 545   Loss: 0.034\n",
      "Epoch: 546   Loss: 0.034\n",
      "Epoch: 547   Loss: 0.034\n",
      "Epoch: 548   Loss: 0.034\n",
      "Epoch: 549   Loss: 0.034\n",
      "Epoch: 550   Loss: 0.034\n",
      "Epoch: 551   Loss: 0.034\n",
      "Epoch: 552   Loss: 0.034\n",
      "Epoch: 553   Loss: 0.034\n",
      "Epoch: 554   Loss: 0.034\n",
      "Epoch: 555   Loss: 0.034\n",
      "Epoch: 556   Loss: 0.034\n",
      "Epoch: 557   Loss: 0.034\n",
      "Epoch: 558   Loss: 0.034\n",
      "Epoch: 559   Loss: 0.034\n",
      "Epoch: 560   Loss: 0.034\n",
      "Epoch: 561   Loss: 0.034\n",
      "Epoch: 562   Loss: 0.034\n",
      "Epoch: 563   Loss: 0.034\n",
      "Epoch: 564   Loss: 0.034\n",
      "Epoch: 565   Loss: 0.034\n",
      "Epoch: 566   Loss: 0.034\n",
      "Epoch: 567   Loss: 0.034\n",
      "Epoch: 568   Loss: 0.034\n",
      "Epoch: 569   Loss: 0.034\n",
      "Epoch: 570   Loss: 0.034\n",
      "Epoch: 571   Loss: 0.034\n",
      "Epoch: 572   Loss: 0.034\n",
      "Epoch: 573   Loss: 0.034\n",
      "Epoch: 574   Loss: 0.034\n",
      "Epoch: 575   Loss: 0.034\n",
      "Epoch: 576   Loss: 0.034\n",
      "Epoch: 577   Loss: 0.034\n",
      "Epoch: 578   Loss: 0.034\n",
      "Epoch: 579   Loss: 0.034\n",
      "Epoch: 580   Loss: 0.034\n",
      "Epoch: 581   Loss: 0.034\n",
      "Epoch: 582   Loss: 0.034\n",
      "Epoch: 583   Loss: 0.034\n",
      "Epoch: 584   Loss: 0.034\n",
      "Epoch: 585   Loss: 0.034\n",
      "Epoch: 586   Loss: 0.034\n",
      "Epoch: 587   Loss: 0.034\n",
      "Epoch: 588   Loss: 0.034\n",
      "Epoch: 589   Loss: 0.034\n",
      "Epoch: 590   Loss: 0.034\n",
      "Epoch: 591   Loss: 0.034\n",
      "Epoch: 592   Loss: 0.034\n",
      "Epoch: 593   Loss: 0.034\n",
      "Epoch: 594   Loss: 0.034\n",
      "Epoch: 595   Loss: 0.034\n",
      "Epoch: 596   Loss: 0.034\n",
      "Epoch: 597   Loss: 0.034\n",
      "Epoch: 598   Loss: 0.034\n",
      "Epoch: 599   Loss: 0.034\n",
      "Epoch: 600   Loss: 0.034\n",
      "Epoch: 601   Loss: 0.034\n",
      "Epoch: 602   Loss: 0.034\n",
      "Epoch: 603   Loss: 0.034\n",
      "Epoch: 604   Loss: 0.034\n",
      "Epoch: 605   Loss: 0.034\n",
      "Epoch: 606   Loss: 0.034\n",
      "Epoch: 607   Loss: 0.034\n",
      "Epoch: 608   Loss: 0.034\n",
      "Epoch: 609   Loss: 0.034\n",
      "Epoch: 610   Loss: 0.034\n",
      "Epoch: 611   Loss: 0.034\n",
      "Epoch: 612   Loss: 0.034\n",
      "Epoch: 613   Loss: 0.034\n",
      "Epoch: 614   Loss: 0.034\n",
      "Epoch: 615   Loss: 0.034\n",
      "Epoch: 616   Loss: 0.034\n",
      "Epoch: 617   Loss: 0.034\n",
      "Epoch: 618   Loss: 0.034\n",
      "Epoch: 619   Loss: 0.034\n",
      "Epoch: 620   Loss: 0.034\n",
      "Epoch: 621   Loss: 0.034\n",
      "Epoch: 622   Loss: 0.034\n",
      "Epoch: 623   Loss: 0.034\n",
      "Epoch: 624   Loss: 0.034\n",
      "Epoch: 625   Loss: 0.034\n",
      "Epoch: 626   Loss: 0.034\n",
      "Epoch: 627   Loss: 0.034\n",
      "Epoch: 628   Loss: 0.034\n",
      "Epoch: 629   Loss: 0.034\n",
      "Epoch: 630   Loss: 0.034\n",
      "Epoch: 631   Loss: 0.034\n",
      "Epoch: 632   Loss: 0.034\n",
      "Epoch: 633   Loss: 0.034\n",
      "Epoch: 634   Loss: 0.034\n",
      "Epoch: 635   Loss: 0.034\n",
      "Epoch: 636   Loss: 0.034\n",
      "Epoch: 637   Loss: 0.034\n",
      "Epoch: 638   Loss: 0.034\n",
      "Epoch: 639   Loss: 0.034\n",
      "Epoch: 640   Loss: 0.034\n",
      "Epoch: 641   Loss: 0.034\n",
      "Epoch: 642   Loss: 0.034\n",
      "Epoch: 643   Loss: 0.034\n",
      "Epoch: 644   Loss: 0.034\n",
      "Epoch: 645   Loss: 0.034\n",
      "Epoch: 646   Loss: 0.034\n",
      "Epoch: 647   Loss: 0.034\n",
      "Epoch: 648   Loss: 0.034\n",
      "Epoch: 649   Loss: 0.034\n",
      "Epoch: 650   Loss: 0.034\n",
      "Epoch: 651   Loss: 0.034\n",
      "Epoch: 652   Loss: 0.034\n",
      "Epoch: 653   Loss: 0.034\n",
      "Epoch: 654   Loss: 0.034\n",
      "Epoch: 655   Loss: 0.034\n",
      "Epoch: 656   Loss: 0.034\n",
      "Epoch: 657   Loss: 0.034\n",
      "Epoch: 658   Loss: 0.034\n",
      "Epoch: 659   Loss: 0.034\n",
      "Epoch: 660   Loss: 0.034\n",
      "Epoch: 661   Loss: 0.034\n",
      "Epoch: 662   Loss: 0.034\n",
      "Epoch: 663   Loss: 0.034\n",
      "Epoch: 664   Loss: 0.034\n",
      "Epoch: 665   Loss: 0.034\n",
      "Epoch: 666   Loss: 0.034\n",
      "Epoch: 667   Loss: 0.034\n",
      "Epoch: 668   Loss: 0.034\n",
      "Epoch: 669   Loss: 0.034\n",
      "Epoch: 670   Loss: 0.034\n",
      "Epoch: 671   Loss: 0.034\n",
      "Epoch: 672   Loss: 0.034\n",
      "Epoch: 673   Loss: 0.034\n",
      "Epoch: 674   Loss: 0.034\n",
      "Epoch: 675   Loss: 0.034\n",
      "Epoch: 676   Loss: 0.034\n",
      "Epoch: 677   Loss: 0.034\n",
      "Epoch: 678   Loss: 0.034\n",
      "Epoch: 679   Loss: 0.034\n",
      "Epoch: 680   Loss: 0.034\n",
      "Epoch: 681   Loss: 0.034\n",
      "Epoch: 682   Loss: 0.034\n",
      "Epoch: 683   Loss: 0.034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 684   Loss: 0.034\n",
      "Epoch: 685   Loss: 0.034\n",
      "Epoch: 686   Loss: 0.034\n",
      "Epoch: 687   Loss: 0.034\n",
      "Epoch: 688   Loss: 0.034\n",
      "Epoch: 689   Loss: 0.034\n",
      "Epoch: 690   Loss: 0.034\n",
      "Epoch: 691   Loss: 0.034\n",
      "Epoch: 692   Loss: 0.034\n",
      "Epoch: 693   Loss: 0.034\n",
      "Epoch: 694   Loss: 0.034\n",
      "Epoch: 695   Loss: 0.034\n",
      "Epoch: 696   Loss: 0.034\n",
      "Epoch: 697   Loss: 0.034\n",
      "Epoch: 698   Loss: 0.034\n",
      "Epoch: 699   Loss: 0.034\n",
      "Epoch: 700   Loss: 0.034\n",
      "Epoch: 701   Loss: 0.034\n",
      "Epoch: 702   Loss: 0.034\n",
      "Epoch: 703   Loss: 0.034\n",
      "Epoch: 704   Loss: 0.034\n",
      "Epoch: 705   Loss: 0.034\n",
      "Epoch: 706   Loss: 0.034\n",
      "Epoch: 707   Loss: 0.034\n",
      "Epoch: 708   Loss: 0.034\n",
      "Epoch: 709   Loss: 0.034\n",
      "Epoch: 710   Loss: 0.034\n",
      "Epoch: 711   Loss: 0.034\n",
      "Epoch: 712   Loss: 0.034\n",
      "Epoch: 713   Loss: 0.034\n",
      "Epoch: 714   Loss: 0.034\n",
      "Epoch: 715   Loss: 0.034\n",
      "Epoch: 716   Loss: 0.034\n",
      "Epoch: 717   Loss: 0.034\n",
      "Epoch: 718   Loss: 0.034\n",
      "Epoch: 719   Loss: 0.034\n",
      "Epoch: 720   Loss: 0.034\n",
      "Epoch: 721   Loss: 0.034\n",
      "Epoch: 722   Loss: 0.034\n",
      "Epoch: 723   Loss: 0.034\n",
      "Epoch: 724   Loss: 0.034\n",
      "Epoch: 725   Loss: 0.034\n",
      "Epoch: 726   Loss: 0.034\n",
      "Epoch: 727   Loss: 0.034\n",
      "Epoch: 728   Loss: 0.034\n",
      "Epoch: 729   Loss: 0.034\n",
      "Epoch: 730   Loss: 0.034\n",
      "Epoch: 731   Loss: 0.034\n",
      "Epoch: 732   Loss: 0.034\n",
      "Epoch: 733   Loss: 0.034\n",
      "Epoch: 734   Loss: 0.034\n",
      "Epoch: 735   Loss: 0.034\n",
      "Epoch: 736   Loss: 0.034\n",
      "Epoch: 737   Loss: 0.034\n",
      "Epoch: 738   Loss: 0.034\n",
      "Epoch: 739   Loss: 0.034\n",
      "Epoch: 740   Loss: 0.034\n",
      "Epoch: 741   Loss: 0.034\n",
      "Epoch: 742   Loss: 0.034\n",
      "Epoch: 743   Loss: 0.034\n",
      "Epoch: 744   Loss: 0.034\n",
      "Epoch: 745   Loss: 0.034\n",
      "Epoch: 746   Loss: 0.034\n",
      "Epoch: 747   Loss: 0.034\n",
      "Epoch: 748   Loss: 0.034\n",
      "Epoch: 749   Loss: 0.034\n",
      "Epoch: 750   Loss: 0.034\n",
      "Epoch: 751   Loss: 0.034\n",
      "Epoch: 752   Loss: 0.033\n",
      "Epoch: 753   Loss: 0.033\n",
      "Epoch: 754   Loss: 0.033\n",
      "Epoch: 755   Loss: 0.033\n",
      "Epoch: 756   Loss: 0.033\n",
      "Epoch: 757   Loss: 0.033\n",
      "Epoch: 758   Loss: 0.033\n",
      "Epoch: 759   Loss: 0.033\n",
      "Epoch: 760   Loss: 0.033\n",
      "Epoch: 761   Loss: 0.033\n",
      "Epoch: 762   Loss: 0.033\n",
      "Epoch: 763   Loss: 0.033\n",
      "Epoch: 764   Loss: 0.033\n",
      "Epoch: 765   Loss: 0.033\n",
      "Epoch: 766   Loss: 0.033\n",
      "Epoch: 767   Loss: 0.033\n",
      "Epoch: 768   Loss: 0.033\n",
      "Epoch: 769   Loss: 0.033\n",
      "Epoch: 770   Loss: 0.033\n",
      "Epoch: 771   Loss: 0.033\n",
      "Epoch: 772   Loss: 0.033\n",
      "Epoch: 773   Loss: 0.033\n",
      "Epoch: 774   Loss: 0.033\n",
      "Epoch: 775   Loss: 0.033\n",
      "Epoch: 776   Loss: 0.033\n",
      "Epoch: 777   Loss: 0.033\n",
      "Epoch: 778   Loss: 0.033\n",
      "Epoch: 779   Loss: 0.033\n",
      "Epoch: 780   Loss: 0.033\n",
      "Epoch: 781   Loss: 0.033\n",
      "Epoch: 782   Loss: 0.033\n",
      "Epoch: 783   Loss: 0.033\n",
      "Epoch: 784   Loss: 0.033\n",
      "Epoch: 785   Loss: 0.033\n",
      "Epoch: 786   Loss: 0.033\n",
      "Epoch: 787   Loss: 0.033\n",
      "Epoch: 788   Loss: 0.033\n",
      "Epoch: 789   Loss: 0.033\n",
      "Epoch: 790   Loss: 0.033\n",
      "Epoch: 791   Loss: 0.033\n",
      "Epoch: 792   Loss: 0.033\n",
      "Epoch: 793   Loss: 0.033\n",
      "Epoch: 794   Loss: 0.033\n",
      "Epoch: 795   Loss: 0.033\n",
      "Epoch: 796   Loss: 0.033\n",
      "Epoch: 797   Loss: 0.033\n",
      "Epoch: 798   Loss: 0.033\n",
      "Epoch: 799   Loss: 0.033\n",
      "Epoch: 800   Loss: 0.033\n",
      "Epoch: 801   Loss: 0.033\n",
      "Epoch: 802   Loss: 0.033\n",
      "Epoch: 803   Loss: 0.033\n",
      "Epoch: 804   Loss: 0.033\n",
      "Epoch: 805   Loss: 0.033\n",
      "Epoch: 806   Loss: 0.033\n",
      "Epoch: 807   Loss: 0.033\n",
      "Epoch: 808   Loss: 0.033\n",
      "Epoch: 809   Loss: 0.033\n",
      "Epoch: 810   Loss: 0.033\n",
      "Epoch: 811   Loss: 0.033\n",
      "Epoch: 812   Loss: 0.033\n",
      "Epoch: 813   Loss: 0.033\n",
      "Epoch: 814   Loss: 0.033\n",
      "Epoch: 815   Loss: 0.033\n",
      "Epoch: 816   Loss: 0.033\n",
      "Epoch: 817   Loss: 0.033\n",
      "Epoch: 818   Loss: 0.033\n",
      "Epoch: 819   Loss: 0.033\n",
      "Epoch: 820   Loss: 0.033\n",
      "Epoch: 821   Loss: 0.033\n",
      "Epoch: 822   Loss: 0.033\n",
      "Epoch: 823   Loss: 0.033\n",
      "Epoch: 824   Loss: 0.033\n",
      "Epoch: 825   Loss: 0.033\n",
      "Epoch: 826   Loss: 0.033\n",
      "Epoch: 827   Loss: 0.033\n",
      "Epoch: 828   Loss: 0.033\n",
      "Epoch: 829   Loss: 0.033\n",
      "Epoch: 830   Loss: 0.033\n",
      "Epoch: 831   Loss: 0.033\n",
      "Epoch: 832   Loss: 0.033\n",
      "Epoch: 833   Loss: 0.033\n",
      "Epoch: 834   Loss: 0.033\n",
      "Epoch: 835   Loss: 0.033\n",
      "Epoch: 836   Loss: 0.033\n",
      "Epoch: 837   Loss: 0.033\n",
      "Epoch: 838   Loss: 0.033\n",
      "Epoch: 839   Loss: 0.033\n",
      "Epoch: 840   Loss: 0.033\n",
      "Epoch: 841   Loss: 0.033\n",
      "Epoch: 842   Loss: 0.033\n",
      "Epoch: 843   Loss: 0.033\n",
      "Epoch: 844   Loss: 0.033\n",
      "Epoch: 845   Loss: 0.033\n",
      "Epoch: 846   Loss: 0.033\n",
      "Epoch: 847   Loss: 0.033\n",
      "Epoch: 848   Loss: 0.033\n",
      "Epoch: 849   Loss: 0.033\n",
      "Epoch: 850   Loss: 0.033\n",
      "Epoch: 851   Loss: 0.033\n",
      "Epoch: 852   Loss: 0.033\n",
      "Epoch: 853   Loss: 0.033\n",
      "Epoch: 854   Loss: 0.033\n",
      "Epoch: 855   Loss: 0.033\n",
      "Epoch: 856   Loss: 0.033\n",
      "Epoch: 857   Loss: 0.033\n",
      "Epoch: 858   Loss: 0.033\n",
      "Epoch: 859   Loss: 0.033\n",
      "Epoch: 860   Loss: 0.033\n",
      "Epoch: 861   Loss: 0.033\n",
      "Epoch: 862   Loss: 0.033\n",
      "Epoch: 863   Loss: 0.033\n",
      "Epoch: 864   Loss: 0.033\n",
      "Epoch: 865   Loss: 0.033\n",
      "Epoch: 866   Loss: 0.033\n",
      "Epoch: 867   Loss: 0.033\n",
      "Epoch: 868   Loss: 0.033\n",
      "Epoch: 869   Loss: 0.033\n",
      "Epoch: 870   Loss: 0.033\n",
      "Epoch: 871   Loss: 0.033\n",
      "Epoch: 872   Loss: 0.033\n",
      "Epoch: 873   Loss: 0.033\n",
      "Epoch: 874   Loss: 0.033\n",
      "Epoch: 875   Loss: 0.033\n",
      "Epoch: 876   Loss: 0.033\n",
      "Epoch: 877   Loss: 0.033\n",
      "Epoch: 878   Loss: 0.033\n",
      "Epoch: 879   Loss: 0.033\n",
      "Epoch: 880   Loss: 0.033\n",
      "Epoch: 881   Loss: 0.033\n",
      "Epoch: 882   Loss: 0.033\n",
      "Epoch: 883   Loss: 0.033\n",
      "Epoch: 884   Loss: 0.033\n",
      "Epoch: 885   Loss: 0.033\n",
      "Epoch: 886   Loss: 0.033\n",
      "Epoch: 887   Loss: 0.033\n",
      "Epoch: 888   Loss: 0.033\n",
      "Epoch: 889   Loss: 0.033\n",
      "Epoch: 890   Loss: 0.033\n",
      "Epoch: 891   Loss: 0.033\n",
      "Epoch: 892   Loss: 0.033\n",
      "Epoch: 893   Loss: 0.033\n",
      "Epoch: 894   Loss: 0.033\n",
      "Epoch: 895   Loss: 0.033\n",
      "Epoch: 896   Loss: 0.033\n",
      "Epoch: 897   Loss: 0.033\n",
      "Epoch: 898   Loss: 0.033\n",
      "Epoch: 899   Loss: 0.033\n",
      "Epoch: 900   Loss: 0.033\n",
      "Epoch: 901   Loss: 0.033\n",
      "Epoch: 902   Loss: 0.033\n",
      "Epoch: 903   Loss: 0.033\n",
      "Epoch: 904   Loss: 0.033\n",
      "Epoch: 905   Loss: 0.033\n",
      "Epoch: 906   Loss: 0.033\n",
      "Epoch: 907   Loss: 0.033\n",
      "Epoch: 908   Loss: 0.033\n",
      "Epoch: 909   Loss: 0.033\n",
      "Epoch: 910   Loss: 0.033\n",
      "Epoch: 911   Loss: 0.033\n",
      "Epoch: 912   Loss: 0.033\n",
      "Epoch: 913   Loss: 0.033\n",
      "Epoch: 914   Loss: 0.033\n",
      "Epoch: 915   Loss: 0.033\n",
      "Epoch: 916   Loss: 0.033\n",
      "Epoch: 917   Loss: 0.033\n",
      "Epoch: 918   Loss: 0.033\n",
      "Epoch: 919   Loss: 0.033\n",
      "Epoch: 920   Loss: 0.033\n",
      "Epoch: 921   Loss: 0.033\n",
      "Epoch: 922   Loss: 0.033\n",
      "Epoch: 923   Loss: 0.033\n",
      "Epoch: 924   Loss: 0.033\n",
      "Epoch: 925   Loss: 0.033\n",
      "Epoch: 926   Loss: 0.033\n",
      "Epoch: 927   Loss: 0.033\n",
      "Epoch: 928   Loss: 0.033\n",
      "Epoch: 929   Loss: 0.033\n",
      "Epoch: 930   Loss: 0.033\n",
      "Epoch: 931   Loss: 0.033\n",
      "Epoch: 932   Loss: 0.033\n",
      "Epoch: 933   Loss: 0.033\n",
      "Epoch: 934   Loss: 0.033\n",
      "Epoch: 935   Loss: 0.033\n",
      "Epoch: 936   Loss: 0.033\n",
      "Epoch: 937   Loss: 0.033\n",
      "Epoch: 938   Loss: 0.033\n",
      "Epoch: 939   Loss: 0.033\n",
      "Epoch: 940   Loss: 0.033\n",
      "Epoch: 941   Loss: 0.033\n",
      "Epoch: 942   Loss: 0.033\n",
      "Epoch: 943   Loss: 0.033\n",
      "Epoch: 944   Loss: 0.033\n",
      "Epoch: 945   Loss: 0.033\n",
      "Epoch: 946   Loss: 0.033\n",
      "Epoch: 947   Loss: 0.033\n",
      "Epoch: 948   Loss: 0.033\n",
      "Epoch: 949   Loss: 0.033\n",
      "Epoch: 950   Loss: 0.033\n",
      "Epoch: 951   Loss: 0.033\n",
      "Epoch: 952   Loss: 0.033\n",
      "Epoch: 953   Loss: 0.033\n",
      "Epoch: 954   Loss: 0.033\n",
      "Epoch: 955   Loss: 0.033\n",
      "Epoch: 956   Loss: 0.033\n",
      "Epoch: 957   Loss: 0.033\n",
      "Epoch: 958   Loss: 0.033\n",
      "Epoch: 959   Loss: 0.033\n",
      "Epoch: 960   Loss: 0.033\n",
      "Epoch: 961   Loss: 0.033\n",
      "Epoch: 962   Loss: 0.033\n",
      "Epoch: 963   Loss: 0.033\n",
      "Epoch: 964   Loss: 0.033\n",
      "Epoch: 965   Loss: 0.033\n",
      "Epoch: 966   Loss: 0.033\n",
      "Epoch: 967   Loss: 0.033\n",
      "Epoch: 968   Loss: 0.033\n",
      "Epoch: 969   Loss: 0.033\n",
      "Epoch: 970   Loss: 0.033\n",
      "Epoch: 971   Loss: 0.033\n",
      "Epoch: 972   Loss: 0.033\n",
      "Epoch: 973   Loss: 0.033\n",
      "Epoch: 974   Loss: 0.033\n",
      "Epoch: 975   Loss: 0.033\n",
      "Epoch: 976   Loss: 0.033\n",
      "Epoch: 977   Loss: 0.033\n",
      "Epoch: 978   Loss: 0.033\n",
      "Epoch: 979   Loss: 0.033\n",
      "Epoch: 980   Loss: 0.033\n",
      "Epoch: 981   Loss: 0.033\n",
      "Epoch: 982   Loss: 0.033\n",
      "Epoch: 983   Loss: 0.033\n",
      "Epoch: 984   Loss: 0.033\n",
      "Epoch: 985   Loss: 0.033\n",
      "Epoch: 986   Loss: 0.033\n",
      "Epoch: 987   Loss: 0.033\n",
      "Epoch: 988   Loss: 0.033\n",
      "Epoch: 989   Loss: 0.033\n",
      "Epoch: 990   Loss: 0.033\n",
      "Epoch: 991   Loss: 0.033\n",
      "Epoch: 992   Loss: 0.033\n",
      "Epoch: 993   Loss: 0.033\n",
      "Epoch: 994   Loss: 0.033\n",
      "Epoch: 995   Loss: 0.033\n",
      "Epoch: 996   Loss: 0.033\n",
      "Epoch: 997   Loss: 0.033\n",
      "Epoch: 998   Loss: 0.033\n",
      "Epoch: 999   Loss: 0.033\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the neural network on the toy dataset\n",
    "losses_nn, params_nn = train(init_neural_network_params, update_neural_network_weights, x_toy, y_true_toy, batch_size=1, num_epoch=1000, learning_rate=0.1, plot_loss=True, lam=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 4.6 Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Loss: 1.368\n",
      "Epoch: 1     Loss: 1.299\n",
      "Epoch: 2     Loss: 1.154\n",
      "Epoch: 3     Loss: 0.924\n",
      "Epoch: 4     Loss: 0.680\n",
      "Epoch: 5     Loss: 0.491\n",
      "Epoch: 6     Loss: 0.364\n",
      "Epoch: 7     Loss: 0.282\n",
      "Epoch: 8     Loss: 0.227\n",
      "Epoch: 9     Loss: 0.188\n",
      "Epoch: 10    Loss: 0.160\n",
      "Epoch: 11    Loss: 0.138\n",
      "Epoch: 12    Loss: 0.121\n",
      "Epoch: 13    Loss: 0.107\n",
      "Epoch: 14    Loss: 0.096\n",
      "Epoch: 15    Loss: 0.087\n",
      "Epoch: 16    Loss: 0.079\n",
      "Epoch: 17    Loss: 0.072\n",
      "Epoch: 18    Loss: 0.066\n",
      "Epoch: 19    Loss: 0.061\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now, we train on the real dataset!\n",
    "losses_nn, params_nn = train(init_neural_network_params, update_neural_network_weights, x, y_true, batch_size=128, num_epoch=20, learning_rate=0.1, plot_loss=True, lam=0.0, plot_per_epoch=False)\n",
    "W1_learned_nn, W2_learned_nn, b1_learned_nn, b2_learned_nn = params_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "losses_nn": [
        1.374822104158618,
        1.3291189161527486,
        1.227850985280715,
        1.0549661599711695,
        0.841585562928492,
        0.6296168800173103,
        0.4601758353551077,
        0.3447294068737151,
        0.268970111700441,
        0.21770411178163035,
        0.1813763655457769,
        0.1545029896887325,
        0.13390793482724292,
        0.11765756924346829,
        0.10453978547547231,
        0.0937347924454232,
        0.0846912617747797,
        0.07701460640640567,
        0.07042499098874866,
        0.06471308160415781
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('losses_nn', losses_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_nn = make_prediction_network(x, W1_learned_nn, b1_learned_nn, W2_learned_nn, b2_learned_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "y_pred_nn.shape": [
        2247
       ]
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('y_pred_nn.shape', y_pred_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy NN: 0.9968847352024922\n"
     ]
    }
   ],
   "source": [
    "accuracy_nn = accuracy_score(dataset.target, y_pred_nn)\n",
    "print('Accuracy NN:', accuracy_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greg/Programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/papermill.record+json": {
       "accuracy.nn": 0.9959946595460614
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "pm.record('accuracy.nn', accuracy_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
